# PCA (Principal Component Analysis) - Подробное объяснение

## Оглавление

1. [Введение](#введение)
2. [Математические основы PCA](#математические-основы-pca)
3. [Матрица ковариации](#матрица-ковариации)
4. [Вычисление PC1 для конкретного WSI](#вычисление-pc1-для-конкретного-wsi)
5. [Важность признаков (Loadings)](#важность-признаков-loadings)
6. [Реализация в коде](#реализация-в-коде)

---

## Введение

**PCA (Principal Component Analysis)** - это метод снижения размерности данных, который находит главные компоненты - направления максимальной вариации в данных.

В нашем проекте PCA используется для:
- Снижения размерности признаков (22 абсолютных или 30 относительных признаков → 1 главная компонента PC1)
- Создания единой шкалы оценки патологии от 0 (норма) до 1 (максимальная патология)
- Выявления важных признаков через loadings

---

## Математические основы PCA

### Шаг 1: Стандартизация данных

Перед применением PCA все признаки стандартизируются (z-score нормализация):

```
X_scaled = (X - μ) / σ
```

Где:
- `X` - исходные значения признаков
- `μ` - среднее значение признака
- `σ` - стандартное отклонение признака

**Зачем это нужно:** Признаки имеют разные масштабы (например, count может быть 0-100, а area - 0-10000). Стандартизация приводит все признаки к одному масштабу, чтобы ни один признак не доминировал из-за больших числовых значений.

### Шаг 2: Применение PCA

PCA находит главные компоненты - направления максимальной вариации в данных.

**Первая главная компонента (PC1)** - это направление, вдоль которого данные варьируются больше всего. Она максимизирует дисперсию и лучше всего разделяет образцы по степени патологии.

---

## Матрица ковариации

### Вычисление матрицы ковариации

PCA можно вычислить двумя эквивалентными способами:

#### Способ 1: Через матрицу ковариации (классический подход)

1. **Вычисление матрицы ковариации:**
   ```
   Cov = (1/(n-1)) × X_scaled^T × X_scaled
   ```
   Где:
   - `X_scaled` - матрица стандартизированных данных (размер: n образцов × p признаков)
   - `n` - число образцов
   - `p` - число признаков
   - `Cov` - матрица ковариации (размер: p × p)

2. **Элементы матрицы ковариации:**
   ```
   Cov[i, j] = (1/(n-1)) × Σ (x_i - μ_i) × (x_j - μ_j)
   ```
   - `Cov[i, i]` - дисперсия i-го признака (диагональные элементы)
   - `Cov[i, j]` - ковариация между признаками i и j (недиагональные элементы)
   - Ковариация показывает, как два признака изменяются вместе

3. **Собственные векторы и собственные значения:**
   ```
   Cov × v = λ × v
   ```
   Где:
   - `v` - собственный вектор (eigenvector) = направление главной компоненты
   - `λ` - собственное значение (eigenvalue) = дисперсия вдоль этого направления
   - Собственные векторы упорядочены по убыванию собственных значений
   - Первый собственный вектор (с наибольшим λ) = PC1

#### Способ 2: Через SVD (Singular Value Decomposition) - используется в sklearn

Sklearn использует более эффективный численный метод - SVD:
```
X_scaled = U × Σ × V^T
```
Где:
- `V^T` - транспонированная матрица правых сингулярных векторов = loadings (components_)
- `Σ` - диагональная матрица сингулярных значений (связана с собственными значениями)
- `U` - матрица левых сингулярных векторов

**Связь между методами:**
- Собственные векторы матрицы ковариации = сингулярные векторы V из SVD
- Собственные значения = квадраты сингулярных значений, деленные на (n-1)
- Оба метода дают одинаковые результаты, но SVD численно более устойчив

### Почему матрица ковариации важна?

1. **Диагональные элементы (дисперсии):**
   - Показывают, насколько каждый признак варьируется
   - Признаки с большой дисперсией потенциально важнее

2. **Недиагональные элементы (ковариации):**
   - Показывают корреляции между признаками
   - Если два признака сильно коррелируют, PCA объединяет их в одну компоненту
   - Это позволяет уменьшить размерность без потери информации

3. **Собственные векторы:**
   - Направления максимальной вариации в данных
   - PC1 = направление наибольшей вариации
   - PC2 = направление второй по величине вариации (ортогонально к PC1)

### Пример вычисления матрицы ковариации

Предположим, у нас есть 3 образца и 2 признака:
```
X_scaled = [[1.0, 0.5],
            [0.0, -0.5],
            [-1.0, 0.0]]
```

Матрица ковариации:
```
Cov = (1/(3-1)) × X_scaled^T × X_scaled
    = 0.5 × [[1.0, 0.0, -1.0],    [[1.0, 0.5],
             [0.5, -0.5, 0.0]]  ×   [0.0, -0.5],
                                     [-1.0, 0.0]]
    = [[1.0, 0.25],
       [0.25, 0.25]]
```

Диагональные элементы: дисперсии признаков (1.0 и 0.25)
Недиагональные элементы: ковариация между признаками (0.25)

---

## Вычисление PC1 для конкретного WSI

После обучения PCA модели, для каждого нового WSI (включая те, на которых обучалась модель) вычисляется PC1 следующим образом:

### Шаг 1: Извлечение признаков WSI

```
X_wsi = [признак₁, признак₂, ..., признакₙ]
```

Например, для WSI "image_001.tif":
```
X_wsi = [Mild_relative_count=0.5, Dysplasia_relative_area=1.2, ..., Paneth_mean_relative_area=0.3]
```

### Шаг 2: Стандартизация признаков WSI

Используются те же параметры стандартизации (μ и σ), что были вычислены при обучении:
```
X_wsi_scaled[i] = (X_wsi[i] - μᵢ) / σᵢ
```

Где:
- `μᵢ` - среднее значение i-го признака из обучающей выборки
- `σᵢ` - стандартное отклонение i-го признака из обучающей выборки

**Важно:** Используются параметры из обучения, а не пересчитываются заново!

### Шаг 3: Вычисление PC1

```
PC1(wsi) = loading₁ × X_wsi_scaled[1] + loading₂ × X_wsi_scaled[2] + ... + loadingₙ × X_wsi_scaled[n]
```

Или в матричной форме:
```
PC1(wsi) = loadings^T × X_wsi_scaled
```

Где `loadings` - вектор loadings первой главной компоненты (из `pca.components_[0]`)

### Шаг 4: Нормализация PC1 (опционально)

Для получения шкалы от 0 до 1:
```
PC1_norm(wsi) = (PC1(wsi) - PC1_min) / (PC1_max - PC1_min)
```

Где `PC1_min` и `PC1_max` - минимальное и максимальное значения PC1 из обучающей выборки

### Пример вычисления PC1 для конкретного WSI

Предположим, у нас есть WSI со следующими признаками:
```
Mild_relative_count = 0.8
Dysplasia_relative_area = 1.5
Crypts_count = 100
```

После стандартизации (используя μ и σ из обучения):
```
Mild_relative_count_scaled = (0.8 - 0.5) / 0.3 = 1.0
Dysplasia_relative_area_scaled = (1.5 - 1.0) / 0.5 = 1.0
Crypts_count_scaled = (100 - 120) / 20 = -1.0
```

Если loadings:
```
Mild_relative_count: loading = +0.25
Dysplasia_relative_area: loading = +0.30
Crypts_count: loading = -0.10
```

Тогда PC1 вычисляется как:
```
PC1(wsi) = (0.25 × 1.0) + (0.30 × 1.0) + (-0.10 × -1.0)
         = 0.25 + 0.30 + 0.10
         = 0.65
```

Если PC1_min = -2.0 и PC1_max = 3.0 из обучающей выборки:
```
PC1_norm(wsi) = (0.65 - (-2.0)) / (3.0 - (-2.0))
             = 2.65 / 5.0
             = 0.53
```

**Интерпретация:** WSI имеет PC1_norm = 0.53, что означает средний уровень патологии (ближе к середине шкалы).

---

## Важность признаков (Loadings)

### Что такое loadings?

**Loadings (коэффициенты загрузки)** - это веса, которые показывают, как каждый признак вносит вклад в PC1.

Loadings берутся из первой строки матрицы `components_` обученной PCA модели:
```python
loadings = pca.components_[0]  # Первая строка = первая главная компонента
```

### Интерпретация loadings

**Абсолютное значение loading** показывает важность признака:
- **Большое абсолютное значение** (например, |0.27|) → признак сильно влияет на PC1
- **Малое абсолютное значение** (например, |0.02|) → признак слабо влияет на PC1

**Знак loading** показывает направление влияния:
- **Положительный loading** (+0.27) → увеличение признака увеличивает PC1 → выше патология
- **Отрицательный loading** (-0.15) → увеличение признака уменьшает PC1 → ниже патология (ближе к норме)

### Почему loadings важны?

PCA автоматически находит оптимальные веса (loadings), которые:
1. **Максимизируют дисперсию** - PC1 объясняет максимальную вариацию в данных
2. **Лучше всего разделяют образцы** - образцы с разной патологией максимально различаются по PC1
3. **Учитывают корреляции** - если признаки коррелируют, PCA это учитывает

Поэтому loadings первой компоненты - это объективная мера важности признаков для разделения норма/патология.

---

## Реализация в коде

### Где это происходит в коде?

В `scale/pca_scoring.py` и `scale/spectral_analysis.py`:

#### Обучение PCA (fit):

```python
# Стандартизация данных
X_scaled = scaler.fit_transform(X)  # Вычисляет μ и σ

# PCA обучение (внутри sklearn использует SVD)
pca = PCA(n_components=None)
pca.fit(X_scaled)  # ← Здесь вычисляется матрица ковариации (через SVD)

# Сохранение параметров
self.scaler = scaler  # Сохраняет mean_ и scale_
self.pca = pca  # Сохраняет components_ (loadings) и explained_variance_
self.pc1_min = X_pca[:, 0].min()  # Минимальное значение PC1
self.pc1_max = X_pca[:, 0].max()  # Максимальное значение PC1
```

#### Применение PCA к новым данным (transform):

```python
# 1. Извлечение признаков
X = df[feature_columns].fillna(0).values

# 2. Стандартизация (используя параметры из обучения)
X_scaled = self.scaler.transform(X)  # ← использует self.scaler.mean_ и self.scaler.scale_

# 3. Вычисление PC1 (используя loadings из обучения)
X_pca = self.pca.transform(X_scaled)  # ← использует self.pca.components_
PC1 = X_pca[:, 0]  # Первая колонка = PC1 для каждого образца

# 4. Нормализация
PC1_norm = (PC1 - self.pc1_min) / (self.pc1_max - self.pc1_min)
```

### Доступ к матрице ковариации

Если нужно явно получить матрицу ковариации из обученной PCA модели:
```python
# Матрица ковариации (если нужна явно)
covariance_matrix = pca.get_covariance()  # Доступна в sklearn PCA

# Или можно вычислить вручную:
import numpy as np
covariance_matrix = np.cov(X_scaled.T)  # Транспонируем для правильной размерности
```

### Получение важности признаков

```python
# Получение loadings (важности признаков)
feature_importance = pca_scorer.get_feature_importance()
# Возвращает pd.Series с loadings, отсортированную по абсолютному значению

# Или напрямую:
loadings = pca.components_[0]  # Первая главная компонента
explained_variance = pca.explained_variance_[0]  # Дисперсия, объясняемая PC1
```

### Ключевой момент

**Все параметры (μ, σ, loadings, PC1_min, PC1_max) фиксируются при обучении (`fit()`) и используются для всех последующих WSI (`transform()`).**

Это гарантирует:
- Единую шкалу для всех образцов
- Сравнимость результатов между разными WSI
- Корректную интерпретацию PC1 значений

---

## Связанные документы

- [docs/FEATURES.md](FEATURES.md) - Описание абсолютных и относительных признаков
- [docs/CLUSTER_SCORING.md](CLUSTER_SCORING.md) - Методы маппинга кластеров на шкалу 0-1
- [docs/KDE_GMM_EXPLANATION.md](KDE_GMM_EXPLANATION.md) - Объяснение спектрального анализа с KDE и GMM







# –ú–µ—Ç–æ–¥—ã –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏: GMM –∏ BIC

## üìä –ú–µ—Ç–æ–¥ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏: Gaussian Mixture Model (GMM)

### –ß—Ç–æ —Ç–∞–∫–æ–µ GMM?

**Gaussian Mixture Model (GMM)** - —ç—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ **—Å–º–µ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö (–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö) —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π**.

### –§–æ—Ä–º—É–ª–∞ GMM:

```
p(x) = Œ£(i=1 to k) w_i √ó N(x | Œº_i, œÉ_i¬≤)
```

–≥–¥–µ:
- `k` - —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–≥–∞—É—Å—Å–æ–≤—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π)
- `w_i` - –≤–µ—Å i-–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (Œ£w_i = 1)
- `Œº_i` - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ i-–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
- `œÉ_i` - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ i-–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
- `N(x | Œº_i, œÉ_i¬≤)` - –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è: Expectation-Maximization (EM)

GMM –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é **EM-–∞–ª–≥–æ—Ä–∏—Ç–º–∞** (Expectation-Maximization):

1. **E-—à–∞–≥ (Expectation)**: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏ –∫ –∫–∞–∂–¥–æ–º—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
2. **M-—à–∞–≥ (Maximization)**: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Œº, œÉ, w) –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è

**–ò—Ç–µ—Ä–∞—Ü–∏–∏**: –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–≤—Ç–æ—Ä—è–µ—Ç E –∏ M —à–∞–≥–∏ –¥–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∫–æ–¥–µ:

```python
from sklearn.mixture import GaussianMixture

# –û–±—É—á–µ–Ω–∏–µ GMM
gmm = GaussianMixture(n_components=n, random_state=42)
gmm.fit(pc1_values)  # EM-–∞–ª–≥–æ—Ä–∏—Ç–º –≤–Ω—É—Ç—Ä–∏
```

### –°—Å—ã–ª–∫–∏ –Ω–∞ GMM:

1. **Scikit-learn –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**
   - https://scikit-learn.org/stable/modules/mixture.html
   - https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html

2. **–ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:**
   - **McLachlan, G., & Peel, D. (2000).** *Finite Mixture Models*. Wiley.
   - **Bishop, C. M. (2006).** *Pattern Recognition and Machine Learning*. Chapter 9: Mixture Models and EM.

3. **EM-–∞–ª–≥–æ—Ä–∏—Ç–º:**
   - **Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977).** Maximum likelihood from incomplete data via the EM algorithm. *Journal of the Royal Statistical Society: Series B*, 39(1), 1-22.
   - https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm

---

## üéØ –ö—Ä–∏—Ç–µ—Ä–∏–π –≤—ã–±–æ—Ä–∞ —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: BIC

### –ß—Ç–æ —Ç–∞–∫–æ–µ BIC?

**BIC (Bayesian Information Criterion)** - —ç—Ç–æ –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ GMM, –∫–æ—Ç–æ—Ä—ã–π **–±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –µ—ë —Å–ª–æ–∂–Ω–æ—Å—Ç—å**.

### –§–æ—Ä–º—É–ª–∞ BIC:

```
BIC = -2 √ó log_likelihood + k √ó log(n)
```

–≥–¥–µ:
- `log_likelihood` - –ª–æ–≥–∞—Ä–∏—Ñ–º –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –º–æ–¥–µ–ª–∏ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
- `k` - —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
  - –î–ª—è GMM —Å k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏: `k = 3k - 1` (k —Å—Ä–µ–¥–Ω–∏—Ö + k –¥–∏—Å–ø–µ—Ä—Å–∏–π + k-1 –≤–µ—Å–æ–≤)
- `n` - —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ (—Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö)
- `log(n)` - —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–ª–æ–≥–∞—Ä–∏—Ñ–º —á–∏—Å–ª–∞ –æ–±—Ä–∞–∑—Ü–æ–≤)

### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:

- **–ú–µ–Ω—å—à–µ BIC = –ª—É—á—à–µ –º–æ–¥–µ–ª—å**
- –ü–µ—Ä–≤–æ–µ —Å–ª–∞–≥–∞–µ–º–æ–µ `-2 √ó log_likelihood`: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–ª–æ—Ö–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∞–Ω–Ω—ã–º (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
- –í—Ç–æ—Ä–æ–µ —Å–ª–∞–≥–∞–µ–º–æ–µ `k √ó log(n)`: —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = –±–æ–ª—å—à–µ —à—Ç—Ä–∞—Ñ)

### –ü–æ—á–µ–º—É BIC –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏?

BIC **—à—Ç—Ä–∞—Ñ—É–µ—Ç –∑–∞ –∫–∞–∂–¥—ã–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä** –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ `log(n)`. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç:
- –ü—Ä–∏ –º–∞–ª–æ–º —á–∏—Å–ª–µ –æ–±—Ä–∞–∑—Ü–æ–≤ (`n`) —à—Ç—Ä–∞—Ñ –º–µ–Ω—å—à–µ
- –ü—Ä–∏ –±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –æ–±—Ä–∞–∑—Ü–æ–≤ (`n`) —à—Ç—Ä–∞—Ñ –±–æ–ª—å—à–µ
- BIC —Å–∫–ª–æ–Ω–µ–Ω –≤—ã–±–∏—Ä–∞—Ç—å **–±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏**, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∫–æ–¥–µ:

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ BIC
best_bic = np.inf
best_n = 1

for n in range(1, max_components + 1):
    gmm = GaussianMixture(n_components=n, random_state=42)
    gmm.fit(pc1_values)
    bic = gmm.bic(pc1_values)  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BIC
    
    if bic < best_bic:
        best_bic = bic
        best_n = n

# –ò—Å–ø–æ–ª—å–∑—É–µ–º best_n –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
```

### –°—Å—ã–ª–∫–∏ –Ω–∞ BIC:

1. **–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è:**
   - **Schwarz, G. (1978).** Estimating the dimension of a model. *Annals of Statistics*, 6(2), 461-464.
   - https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-dimension-of-a-model/10.1214/aos/1176344136.full

2. **–û–±–∑–æ—Ä –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å AIC:**
   - **Burnham, K. P., & Anderson, D. R. (2004).** *Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach*. Springer.
   - https://en.wikipedia.org/wiki/Bayesian_information_criterion

3. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ GMM:**
   - **Fraley, C., & Raftery, A. E. (2002).** Model-based clustering, discriminant analysis, and density estimation. *Journal of the American Statistical Association*, 97(458), 611-631.
   - https://www.jstor.org/stable/3085674

---

## üîÑ –°–≤—è–∑—å –º–µ–∂–¥—É GMM –∏ BIC

### –ü—Ä–æ—Ü–µ—Å—Å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏:

1. **GMM** - —ç—Ç–æ –º–µ—Ç–æ–¥ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ (–∫–∞–∫ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ)
2. **BIC** - —ç—Ç–æ –∫—Ä–∏—Ç–µ—Ä–∏–π –≤—ã–±–æ—Ä–∞ (—Å–∫–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)

### –ê–ª–≥–æ—Ä–∏—Ç–º –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:

```
–î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ k –æ—Ç 1 –¥–æ max_components:
    1. –û–±—É—á–∏—Ç—å GMM —Å k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ (EM-–∞–ª–≥–æ—Ä–∏—Ç–º)
    2. –í—ã—á–∏—Å–ª–∏—Ç—å BIC –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
    3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å k —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º BIC

–í—ã–±—Ä–∞—Ç—å k —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º BIC
```

### –ü—Ä–∏–º–µ—Ä –∏–∑ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö:

```
–ß–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤    BIC        RMSE      R¬≤
1                   187.58     0.02285   0.6958
2                   185.58     0.02434   0.6549  ‚Üê BIC –≤—ã–±–∏—Ä–∞–µ—Ç —ç—Ç–æ
3                   185.36     0.04514   -0.1871 ‚Üê –ù–æ RMSE —Ö—É–∂–µ!
```

**–ü–∞—Ä–∞–¥–æ–∫—Å**: BIC –≤—ã–±—Ä–∞–ª 3 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (185.36 < 185.58), –Ω–æ RMSE –ª—É—á—à–µ —Å 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏!

### –ü–æ—á–µ–º—É —Ç–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç?

1. **BIC —à—Ç—Ä–∞—Ñ—É–µ—Ç –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å**, –Ω–æ –ø—Ä–∏ –º–∞–ª–æ–º —á–∏—Å–ª–µ –æ–±—Ä–∞–∑—Ü–æ–≤ (36) —à—Ç—Ä–∞—Ñ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º
2. **RMSE –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å** –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (KDE)
3. **3 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –º–æ–≥—É—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è** –Ω–∞ –º–∞–ª–æ–º —á–∏—Å–ª–µ –æ–±—Ä–∞–∑—Ü–æ–≤

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:

- **–î–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ª–µ–π**: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å **–ª—É—á—à–∏–º RMSE** (2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞)
- **–î–ª—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞**: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BIC, –Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –ö–Ω–∏–≥–∏:

1. **Bishop, C. M. (2006).** *Pattern Recognition and Machine Learning*. 
   - –ì–ª–∞–≤–∞ 9: Mixture Models and EM
   - https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf

2. **Murphy, K. P. (2012).** *Machine Learning: A Probabilistic Perspective*.
   - –ì–ª–∞–≤–∞ 11: Mixture Models and the EM Algorithm
   - https://probml.github.io/pml-book/book1.html

### –û–Ω–ª–∞–π–Ω –∫—É—Ä—Å—ã:

1. **Coursera: Machine Learning (Andrew Ng)**
   - Week 8: Clustering & Mixture Models
   - https://www.coursera.org/learn/machine-learning

2. **edX: MIT 6.034 Artificial Intelligence**
   - Lecture 20: Learning: Clustering & Mixture Models
   - https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:

1. **Distill.pub: Understanding the EM Algorithm**
   - https://distill.pub/2017/em-guide/

2. **Observable: Gaussian Mixture Models**
   - https://observablehq.com/@mbostock/gaussian-mixture-model

---

## üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏

### AIC (Akaike Information Criterion):

```
AIC = -2 √ó log_likelihood + 2k
```

- **–ú–µ–Ω—å—à–∏–π —à—Ç—Ä–∞—Ñ** –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å (2k –≤–º–µ—Å—Ç–æ k√ólog(n))
- **–°–∫–ª–æ–Ω–µ–Ω –≤—ã–±–∏—Ä–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤**, —á–µ–º BIC
- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è **–±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫**

### Cross-Validation:

- –†–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/validation
- –û–±—É—á–∏—Ç—å GMM –Ω–∞ train
- –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ validation
- –í—ã–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π –æ—Ü–µ–Ω–∫–æ–π

### –°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤:

- **Burnham, K. P., & Anderson, D. R. (2004).** *Model Selection and Multimodel Inference*
- **Vrieze, S. I. (2012).** Model selection and psychological theory: a discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). *Psychological Methods*, 17(2), 228-243.





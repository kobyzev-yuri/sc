# План реализации кластерного анализа для шкалы отклонения от нормы

## Обзор проекта

Проект направлен на создание системы кластерного анализа патологических данных из Whole Slide Images (WSI) биоптатов для построения интерпретируемой шкалы оценки патологии от 0 до 1.

## Текущее состояние

✅ **Реализовано:**
- Обработка WSI изображений (`wsi.py`)
- Детекция патологий с помощью YOLO моделей (`aimodels.py`)
- Предсказания по WSI с постобработкой (`predict.py`)
- Доменные модели (`domain.py`)
- Визуализация предсказаний (`visualize.py`)
- ✅ Конфигурация моделей (`model_config.py`) - создан модуль для удобной загрузки всех моделей

✅ **Реализовано в ноутбуках (требуется перенос в модули):**

**notebook/predict.ipynb:**
- Предсказания патологий на WSI изображениях
- Сохранение предсказаний в JSON формат

**notebook/analyze.ipynb:**
- ✅ Агрегация предсказаний в DataFrame с count/area признаками (Cell 6)
- ✅ Создание относительных признаков:
  - `relative_count = count / Crypts_count`
  - `relative_area = area / Crypts_area`
  - `mean_relative_area = relative_area / count` (Cell 8)
- ✅ PCA анализ с выделением главных компонент (Cell 12)
- ✅ Создание шкалы PC1_norm (нормализованная PC1 от 0 до 1) (Cell 12)
- ✅ Разметка данных на группы:
  - `mild`: 5 образцов с умеренными изменениями
  - `norma`: 5 нормальных образцов (Cell 11, 13)
- ✅ Выявление важных признаков через loadings PC1:
  - Dysplasia_mean_relative_area (0.272)
  - Mild_relative_area (0.263)
  - Mild_relative_count (0.247)
  - Neutrophils_relative_area (0.246)
  - И другие (Cell 12 output)

✅ **Реализовано дополнительно:**
- ✅ Кластеризация (HDBSCAN, Agglomerative, KMeans) - `scale/clustering.py`
- ✅ Препроцессинг данных (анализ корреляций, удаление избыточных признаков) - `scale/preprocessing.py`
- ✅ EDA (визуализации, корреляции, UMAP, статистические тесты) - `scale/eda.py`
- ✅ Кластерный маппинг на шкалу 0-1 - `scale/cluster_scoring.py`
- ✅ Сравнение нескольких кластеризаций - `scale/cluster_comparison.py`
- ✅ Сравнение разных методов построения шкалы (PCA/Spectral/Cluster) - `scale/method_comparison.py`
- ✅ Веб-дашборд с полной интеграцией всех методов - `scale/dashboard.py`

❌ **Не реализовано (опционально):**
- Anomaly Detection для построения шкалы
- Supervised Regression (LightGBM)
- Интерпретация через SHAP

## План реализации

### Этап 0: Перенос кода из ноутбуков в модули проекта ✅

**Статус:** Завершен. Код перенесен в модули.

**Что уже сделано в ноутбуках:**

**notebook/analyze.ipynb:**
1. **Агрегация предсказаний** (Cell 6):
   - Загрузка JSON файлов с предсказаниями
   - Подсчет count и area для каждого типа патологии
   - Создание DataFrame с признаками

2. **Создание относительных признаков** (Cell 8):
   - Нормализация по Crypts (count/Crypts_count, area/Crypts_area)
   - Вычисление mean_relative_area
   - Отбор важных признаков

3. **PCA анализ** (Cell 12):
   - StandardScaler для нормализации
   - PCA для снижения размерности
   - Вычисление loadings для интерпретации
   - Создание шкалы PC1_norm (0-1)

4. **Разметка данных** (Cell 11, 13):
   - Группа `mild`: 5 образцов
   - Группа `norma`: 5 образцов
   - Создание меток для сравнения

**Выполнено:**
1. ✅ Создан модуль `aggregate.py`:
   - `aggregate_predictions_from_dict()` - агрегация из словаря предсказаний
   - `aggregate_predictions_from_json()` - агрегация из JSON файла
   - `load_predictions_batch()` - загрузка всех предсказаний из директории
   - `create_relative_features()` - создание относительных признаков (нормализация по Crypts)
   - `select_feature_columns()` - отбор важных признаков

2. ✅ Создан модуль `pca_scoring.py`:
   - Класс `PCAScorer` для PCA анализа и создания шкалы
   - Метод `fit()` - обучение StandardScaler и PCA
   - Метод `transform()` - преобразование данных и создание PC1_norm
   - Метод `fit_transform()` - обучение и преобразование
   - Метод `get_feature_importance()` - получение loadings для интерпретации
   - Методы `save()` и `load()` - сохранение/загрузка модели

3. ✅ Старые README файлы перемещены в `archive/`

**Файлы:**
- `scale/model_config.py` - ✅ создан (конфигурация моделей)
- `scale/aggregate.py` - ✅ создан (агрегация предсказаний)
- `scale/pca_scoring.py` - ✅ создан (PCA анализ и шкала)

**Ресурсы:**
- `models/` - ✅ все 11 моделей YOLO уже присутствуют
- `wsi/` - директория для WSI изображений (пока 1 файл: `17_hp_S008__20240820_084008.tiff`)

---

### Этап 1: Агрегация предсказаний в таблицу признаков ✅

**Статус:** Завершен. Реализовано в `scale/aggregate.py`.

**Цель:** Преобразовать предсказания патологий в таблицу с количественными признаками (count и area для каждого типа патологии).

**Реализовано:**
- ✅ `aggregate_predictions_from_json()` - агрегация из JSON файла
- ✅ `aggregate_predictions_from_dict()` - агрегация из словаря предсказаний
- ✅ `load_predictions_batch()` - загрузка всех предсказаний из директории
- ✅ `create_relative_features()` - создание относительных признаков (нормализация по Crypts)
- ✅ `select_feature_columns()` - отбор важных признаков
   
   **Включенные признаки:**
   - Mild, Dysplasia, Moderate, Meta, Plasma Cells, Neutrophils, EoE, Enterocytes, Granulomas
   - **Paneth** ✅ (добавлен в версии с поддержкой Paneth клеток)
   - Surface epithelium, Muscularis mucosae
   - Для каждого типа: `relative_count`, `relative_area`, `mean_relative_area`

**Выходные данные:**
```csv
image,Mild_count,Mild_area,Crypts_count,Crypts_area,...,Dysplasia_count,Dysplasia_area,...
9_ibd_mod_4mod,21,4718158.0,76,12431430.0,...,20,...
```

**Файлы:**
- `scale/aggregate.py` - ✅ модуль для агрегации

---

### Этап 2: Подготовка данных для кластерного анализа ✅

**Статус:** Завершен. Реализовано в `scale/preprocessing.py`.

**Цель:** Подготовить данные для машинного обучения (нормализация, вычисление плотностей, удаление избыточных признаков).

**Что уже сделано:**
- ✅ Создание относительных признаков (нормализация по Crypts)
- ✅ StandardScaler для нормализации (используется в PCA)
- ✅ Обработка NaN (fillna(0))

**Реализовано:**
- ✅ Анализ корреляций между признаками (`find_highly_correlated_features`)
- ✅ Удаление избыточных признаков (`remove_redundant_features`)
- ✅ Визуализация корреляционной матрицы (`visualize_correlations`)
- ✅ Визуализация распределений признаков (`visualize_feature_distributions`)

**Файлы:**
- `scale/preprocessing.py` - ✅ модуль для препроцессинга

---

### Этап 3: Разведочный анализ данных (EDA) ✅

**Статус:** Завершен. Реализовано в `scale/eda.py`.

**Цель:** Понять структуру данных, распределения признаков, выявить паттерны.

**Реализовано:**
- ✅ Визуализация распределений по группам (`visualize_distributions_by_group`)
- ✅ PCA scatter plot визуализация (`visualize_pca_scatter`)
- ✅ UMAP визуализация (`visualize_umap`)
- ✅ Статистические тесты (t-test, Mann-Whitney U test) (`statistical_tests`)

**Файлы:**
- `scale/eda.py` - ✅ модуль для EDA

---

### Этап 4: Кластеризация ✅

**Статус:** Завершен. Реализовано в `scale/clustering.py`.

**Цель:** Выявить скрытые паттерны в данных и идентифицировать патологические фенотипы.

**Реализовано:**
- ✅ HDBSCAN кластеризация (автоматическое определение числа кластеров)
- ✅ Альтернативные методы: Agglomerative Clustering, KMeans
- ✅ Визуализация кластеров в UMAP-пространстве
- ✅ Анализ средних значений признаков в каждом кластере
- ✅ Автоматическая интерпретация кластеров
- ✅ Метрики качества кластеризации (silhouette score, Calinski-Harabasz, Davies-Bouldin)
- ✅ Сохранение/загрузка моделей

**Файлы:**
- `scale/clustering.py` - ✅ модуль для кластеризации
- `examples/test_clustering.py` - пример использования

**Зависимости:**
- hdbscan
- umap-learn

---

### Этап 5: Построение шкалы 0-1 ✅ (расширено спектральным анализом)

**Статус:** Базовая PCA шкала реализована, добавлен спектральный анализ для универсальности.

**Цель:** Создать интерпретируемую шкалу оценки патологии от 0 (норма) до 1 (максимальное отклонение).

**Что уже сделано:**
- ✅ PCA анализ с созданием PC1_norm (нормализованная PC1 от 0 до 1)
- ✅ Выявление важных признаков через loadings
- ✅ Разметка данных (mild/norma) для валидации
- ✅ Спектральный анализ для выявления стабильных состояний (мод)

**Реализовано:**

1. **Модуль `pca_scoring.py`** (базовый PCA):
   - Класс `PCAScorer`:
     - `fit()` - обучение StandardScaler и PCA
     - `transform()` - преобразование данных и создание PC1_norm
     - `get_feature_importance()` - получение loadings для интерпретации
   - Сохранение модели (scaler + pca)

2. **Модуль `spectral_analysis.py`** ✅ (расширенный спектральный анализ):
   - Класс `SpectralAnalyzer`:
     - `fit_pca()` - обучение PCA для снижения размерности
     - `transform_pca()` - преобразование через PCA
     - `fit_spectrum()` - анализ спектра с использованием процентилей (P1, P99)
     - `_find_modes_kde()` - поиск мод через Kernel Density Estimation
     - `fit_gmm()` - обучение Gaussian Mixture Model для моделирования состояний
     - `transform_to_spectrum()` - преобразование в спектральную шкалу 0-1 с классификацией
     - `get_spectrum_info()` - получение информации о спектре
     - `visualize_spectrum()` - визуализация спектра на шкале
     - `save()` и `load()` - сохранение/загрузка модели
   
   **Особенности:**
   - Использование процентилей (P1, P99) для буферных зон
   - Автоматическое выявление мод (стабильных состояний) через KDE
   - Поддержка GMM для моделирования смеси состояний
   - Визуализация спектра с отображением мод на шкале 0-1
   - Автоматическая классификация образцов на основе спектральной шкалы
   
   **Классификация образцов (PC1_mode):**
   
   Классификация основана на позиции образца на спектральной шкале (0-1):
   
   | Спектральная шкала | Класс | Описание |
   |-------------------|-------|----------|
   | **0.0 - 0.2** | `normal` | Нормальные образцы |
   | **0.2 - 0.5** | `mild` | Легкая патология |
   | **0.5 - 0.8** | `moderate` | Умеренная патология |
   | **0.8 - 1.0** | `severe` | Тяжелая патология |
   
   **Формула преобразования PC1 → Spectrum:**
   ```
   PC1_spectrum = (PC1 - PC1_p1) / (PC1_p99 - PC1_p1)
   ```
   
   где:
   - `PC1_p1` = 1-й процентиль PC1 (нижняя граница, по умолчанию)
   - `PC1_p99` = 99-й процентиль PC1 (верхняя граница, по умолчанию)
   
   **Примеры порогов** (для типичных данных):
   - Spectrum 0.0 → PC1 ≈ -3.4 (начало normal)
   - Spectrum 0.2 → PC1 ≈ -0.9 (граница normal/mild)
   - Spectrum 0.5 → PC1 ≈ 2.9 (граница mild/moderate)
   - Spectrum 0.8 → PC1 ≈ 6.7 (граница moderate/severe)
   - Spectrum 1.0 → PC1 ≈ 9.2 (конец severe)
   
   **Дополнительные колонки в результатах:**
   - `PC1_spectrum` - позиция на спектральной шкале (0-1)
   - `PC1_mode` - классификация (normal/mild/moderate/severe)
   - `PC1_mode_distance` - расстояние до ближайшей моды (для справки)
   - `PC1_nearest_mode` - метка ближайшей моды (для справки)
   
   Подробнее см. [docs/CLASSIFICATION_CRITERIA.md](docs/CLASSIFICATION_CRITERIA.md)

**Вариант A: Anomaly Detection** (для сравнения)
1. Модуль `anomaly_detection.py`:
   - Обучение на нормальных данных (unsupervised):
     - Isolation Forest
     - One-Class SVM
     - Autoencoder (MAE reconstruction loss)
   - Вычисление anomaly score → нормализация в [0, 1]
   - Сравнение с PCA методом

**Вариант B: Supervised Regression** (если есть разметка)
1. Модуль `regression.py`:
   - LightGBM Regressor
   - SHAP для интерпретации
   - Сравнение с PCA методом

**Вариант C: Кластер → Score Mapping** ✅ (после кластеризации)
1. Модуль `cluster_scoring.py`:
   - ✅ Маппинг кластеров на score (0.0 - 1.0)
   - ✅ 5 методов маппинга:
     - `spectrum_projection` ⭐ - интегрированный подход со спектральным анализом (единая шкала, моды)
     - `pathology_features` - на основе патологических признаков
     - `pc1_centroid` - на основе PC1 центроидов
     - `expert_labels` - на основе экспертной разметки
     - `distance_from_normal` - на основе расстояния от нормального кластера
   - ✅ Интеграция с результатами кластеризации
   - ✅ Нормализация через процентили (P1/P99) для устойчивости к выбросам
   - ✅ Учет распределений внутри кластеров (медиана, процентили)
   - ✅ Классификация кластеров по модам из спектрального анализа
   - ✅ Полная интеграция в веб-дашборд
   
   **Документация:** [docs/CLUSTER_SCORING.md](docs/CLUSTER_SCORING.md)

**Рекомендация:** 
- Начать с переноса PCA метода из ноутбука (уже работает)
- Добавить кластеризацию (Этап 4) → Вариант C
- Затем сравнить с Anomaly Detection (Вариант A)

**Файлы:**
- `scale/pca_scoring.py` - ✅ базовая PCA шкала
- `scale/spectral_analysis.py` - ✅ спектральный анализ с модами (расширенный)
- `scale/cluster_scoring.py` - ✅ вариант C (кластерный маппинг)
- `scale/cluster_comparison.py` - ✅ сравнение нескольких кластеризаций
- `scale/method_comparison.py` - ✅ сравнение PCA/Spectral/Cluster методов
- `scale/anomaly_detection.py` - вариант A (новый, опционально)
- `scale/regression.py` - вариант B (новый, опционально)
- `models/spectral_analyzer.pkl` - сохраненная модель спектрального анализа

**Зависимости:**
- lightgbm (для варианта B)
- shap (для интерпретации)

---

### Этап 6: Интерпретация и визуализация ✅ (дашборд создан)

**Статус:** Веб-дашборд создан, интерпретация через SHAP - опционально.

**Цель:** Создать инструменты для интерпретации результатов и визуализации.

**Реализовано:**

1. **Модуль `dashboard.py`** ✅ (веб-интерфейс на Streamlit):
   - Загрузка JSON файлов с предсказаниями через веб-интерфейс
   - Агрегация данных и создание признаков
   - Выбор признаков (blacklist/whitelist режимы)
   - Визуализация распределений признаков
   - Спектральный анализ с визуализацией
   - Анализ конкретных образцов с рекомендациями
   - Кластеризация с визуализацией и интерпретацией
   - **Сравнение нескольких кластеризаций** с автоматической рекомендацией лучшего
   - **Сравнение разных методов построения шкалы** (PCA/Spectral/Cluster)
   - **Метрики качества набора признаков** с подробными пояснениями:
     - Score (комплексная оценка: 40% separation + 30% mod позиция + 30% дисперсия)
     - Separation (разделение между mod и normal образцами)
     - Mod (норм. PC1) - позиция патологических образцов на шкале 0-1
     - Объясненная дисперсия PC1
     - Автоматическое вычисление метрик для пользовательских данных
   - Отображение важности признаков (PC1 loadings)
   - Корреляционный анализ с удалением избыточных признаков
   - Сохранение экспериментов в директорию `experiments/`
   - Скачивание результатов в CSV
   - Настройки через боковую панель:
     - Использование относительных признаков
     - Спектральный анализ
     - Настройка процентилей
     - Сравнение методов построения шкалы
   
   **Документация:** [examples/README_DASHBOARD.md](examples/README_DASHBOARD.md)


**Задачи (опционально):**
1. Модуль `interpretation.py`:
   - SHAP значения для важности признаков
   - Визуализация вклада каждого признака в итоговый score
   - Генерация объяснений для каждого предсказания

**Файлы:**
- `scale/dashboard.py` - ✅ веб-дашборд (Streamlit)
- `scale/interpretation.py` - интерпретация через SHAP (опционально)

**Зависимости:**
- streamlit ✅
- matplotlib ✅
- shap (опционально, для расширенной интерпретации)

**Использование:**
```bash
# Установка зависимостей
pip install streamlit matplotlib

# Запуск дашборда
streamlit run scale/dashboard.py
```

---

### Этап 7: Интеграция и тестирование

**Цель:** Интегрировать все модули в единый pipeline.

**Задачи:**
1. Создать главный скрипт `analyze.py`:
   - Pipeline от предсказаний до получения score
   - Обработка одного или множества WSI
   - Сохранение результатов

2. Создать примеры использования:
   - `examples/basic_analysis.py` - базовый пример
   - `examples/batch_processing.py` - обработка множества файлов

3. Тестирование:
   - Unit тесты для ключевых функций
   - Интеграционные тесты для pipeline

**Файлы:**
- `scale/analyze.py` - главный скрипт анализа
- `examples/` - папка с примерами
- `tests/` - папка с тестами

---

## Структура файлов после реализации

```
scale/
├── __init__.py
├── domain.py              ✅ (существует)
├── aimodels.py            ✅ (существует)
├── wsi.py                 ✅ (существует)
├── predict.py             ✅ (существует)
├── visualize.py           ✅ (существует)
├── model_config.py        ✅ (создан)
├── aggregate.py           ✅ (создан)
├── pca_scoring.py         ✅ (создан)
├── spectral_analysis.py   ✅ (создан)
├── preprocessing.py       ✅ (создан)
├── eda.py                 ✅ (создан)
├── clustering.py          ✅ (создан)
├── cluster_scoring.py     ✅ (создан)
├── cluster_comparison.py  ✅ (создан)
├── method_comparison.py   ✅ (создан)
├── dashboard.py           ✅ (создан)
├── scale_comparison.py    ✅ (создан)
├── anomaly_detection.py   ❌ (новый, опционально)
├── regression.py          ❌ (новый, опционально)
├── interpretation.py      ❌ (новый, опционально)
└── analyze.py             ❌ (новый)

examples/
├── test_with_predictions.py    ✅ (создан)
├── test_clustering.py          ✅ (создан)
├── compare_clusterings.py      ✅ (создан)
├── README_DASHBOARD.md         ✅ (создан)
└── README_TESTING.md          ✅ (создан)

docs/
└── CLUSTER_SCORING.md          ✅ (создан)

experiments/                ✅ (создается автоматически)

models/
├── *.pt                   ✅ (существуют)
└── clusterer.pkl          ✅ (создается при сохранении)

results/
├── predictions/           ✅ (JSON файлы с предсказаниями)
└── visualization/         ✅ (графики и результаты)
```

---

## Зависимости для установки

```bash
pip install pandas numpy scikit-learn umap-learn hdbscan lightgbm shap seaborn matplotlib plotly
```

Опционально:
```bash
pip install streamlit  # для дашборда
```

---

## Порядок реализации (рекомендуемый)

0. **Этап 0** - Перенос кода из ноутбуков ✅ **ЗАВЕРШЕН**
   - ✅ Создан `aggregate.py` - агрегация предсказаний и создание относительных признаков
   - ✅ Создан `pca_scoring.py` - PCA анализ и создание шкалы PC1_norm
   - ✅ Старые README перемещены в архив

1. **Этап 1** - Агрегация предсказаний ✅ **ЗАВЕРШЕН**
   - ✅ Реализовано в `aggregate.py`

2. **Этап 2** - Подготовка данных ✅ **ЗАВЕРШЕН**
   - ✅ Анализ корреляций и удаление избыточных признаков
   - ✅ Визуализация корреляций и распределений

3. **Этап 3** - EDA ✅ **ЗАВЕРШЕН**
   - ✅ Визуализация распределений по группам
   - ✅ PCA и UMAP визуализация
   - ✅ Статистические тесты

4. **Этап 4** - Кластеризация ✅ **ЗАВЕРШЕН**
   - ✅ HDBSCAN, Agglomerative, KMeans кластеризация
   - ✅ Автоматическая интерпретация кластеров
   - ✅ Метрики качества и визуализация

5. **Этап 5** - Построение шкалы ✅ **РАСШИРЕН**
   - ✅ Базовый PCA метод в `pca_scoring.py`
   - ✅ Спектральный анализ в `spectral_analysis.py` (моды, процентили, визуализация)
   - ✅ Кластерный маппинг в `cluster_scoring.py` (4 метода)
   - Опционально: альтернативные методы (anomaly detection, regression)

6. **Этап 6** - Интерпретация и визуализация ✅ **ДАШБОРД СОЗДАН**
   - ✅ Веб-интерфейс на Streamlit для загрузки данных и визуализации
   - ✅ Сохранение экспериментов
   - Опционально: SHAP значения для расширенной интерпретации

7. **Этап 7** - Интеграция ❌ (новое)
   - Объединение всех модулей в pipeline
   - Создание главного скрипта `analyze.py`

---

## Важные замечания

1. **Нет площади ткани на слайде?** → Использовать нормализацию по максимальному признаку или `area / white_space_area` как proxy.

2. **Мало данных «нормы»?** → Использовать semi-supervised подход: норма = кластер с минимальной активностью всех маркеров.

3. **Нет ground truth диагнозов?** → Начать с unsupervised кластеризации → показать патоморфологу репрезентативные слайды из каждого кластера → получить разметку.

4. **Калибровка модели:** После вердикта патоморфолога можно обновлять модель (online learning).

---

## Документация

- **[docs/FEATURES.md](docs/FEATURES.md)** - Подробное описание абсолютных и относительных признаков, их количества и формул
- **[docs/PCA.md](docs/PCA.md)** - Подробное объяснение PCA: матрица ковариации, вычисление PC1 для WSI, важность признаков (loadings)
- **[docs/CLUSTER_SCORING.md](docs/CLUSTER_SCORING.md)** - Подробное описание методов маппинга кластеров на шкалу 0-1
- **[examples/README_DASHBOARD.md](examples/README_DASHBOARD.md)** - Руководство по использованию веб-дашборда
- **[examples/README_TESTING.md](examples/README_TESTING.md)** - Примеры использования модулей
- **[docs/ANALYSIS.md](docs/ANALYSIS.md)** - Анализ результатов и известные проблемы
- **[docs/CLASSIFICATION_CRITERIA.md](docs/CLASSIFICATION_CRITERIA.md)** - Критерии классификации образцов (normal/mild/moderate/severe) и пороги на спектральной шкале
- **[docs/HOW_TO_RERUN_ANALYSIS.md](docs/HOW_TO_RERUN_ANALYSIS.md)** - Как перезапустить анализ в веб-интерфейсе

## Новые возможности

### Сравнение кластеризаций
- Загрузка нескольких сохраненных кластеризаторов
- Сравнение метрик качества (Silhouette, Calinski-Harabasz, Davies-Bouldin)
- Автоматическая рекомендация лучшего кластеризатора
- Маппинг всех кластеризаций на шкалу 0-1
- Визуализация сравнения

### Сравнение методов построения шкалы
- Сравнение PCA Scoring, Spectral Analysis и Cluster-based Scoring
- Вычисление корреляций (Pearson, Spearman)
- Схожесть распределений (Jensen-Shannon divergence)
- Визуализация сравнения (6 графиков)
- Рекомендация лучшего метода по критериям

## Следующие шаги

1. ✅ Подготовить CSV таблицу с данными - использовать `aggregate.py`
2. ✅ Провести разведочный анализ данных - использовать `eda.py`
3. ✅ Реализовать кластеризацию (HDBSCAN) - использовать `clustering.py`
4. ✅ Построить шкалу оценки патологии - использовать `cluster_scoring.py`
5. ✅ Создать интерпретируемый дашборд - использовать `dashboard.py`
6. ❌ Интеграция всех модулей в единый pipeline (`analyze.py`)
7. ❌ Unit и интеграционные тесты
8. ❌ Опционально: Anomaly Detection и Supervised Regression методы


# План реализации кластерного анализа для шкалы отклонения от нормы

## Обзор проекта

Проект направлен на создание системы кластерного анализа патологических данных из Whole Slide Images (WSI) биоптатов для построения интерпретируемой шкалы оценки патологии от 0 до 1.

## Текущее состояние

✅ **Реализовано:**
- Обработка WSI изображений (`wsi.py`)
- Детекция патологий с помощью YOLO моделей (`aimodels.py`)
- Предсказания по WSI с постобработкой (`predict.py`)
- Доменные модели (`domain.py`)
- Визуализация предсказаний (`visualize.py`)
- ✅ Конфигурация моделей (`model_config.py`) - создан модуль для удобной загрузки всех моделей

✅ **Реализовано в ноутбуках (требуется перенос в модули):**

**notebook/predict.ipynb:**
- Предсказания патологий на WSI изображениях
- Сохранение предсказаний в JSON формат

**notebook/analyze.ipynb:**
- ✅ Агрегация предсказаний в DataFrame с count/area признаками (Cell 6)
- ✅ Создание относительных признаков:
  - `relative_count = count / Crypts_count`
  - `relative_area = area / Crypts_area`
  - `mean_relative_area = relative_area / count` (Cell 8)
- ✅ PCA анализ с выделением главных компонент (Cell 12)
- ✅ Создание шкалы PC1_norm (нормализованная PC1 от 0 до 1) (Cell 12)
- ✅ Разметка данных на группы:
  - `mild`: 5 образцов с умеренными изменениями
  - `norma`: 5 нормальных образцов (Cell 11, 13)
- ✅ Выявление важных признаков через loadings PC1:
  - Dysplasia_mean_relative_area (0.272)
  - Mild_relative_area (0.263)
  - Mild_relative_count (0.247)
  - Neutrophils_relative_area (0.246)
  - И другие (Cell 12 output)

❌ **Не реализовано:**
- Кластеризация (HDBSCAN)
- Расширенный EDA (визуализации, корреляции)
- Альтернативные методы построения шкалы (anomaly detection, regression)
- Интерпретация через SHAP

## План реализации

### Этап 0: Перенос кода из ноутбуков в модули проекта ✅

**Статус:** Завершен. Код перенесен в модули.

**Что уже сделано в ноутбуках:**

**notebook/analyze.ipynb:**
1. **Агрегация предсказаний** (Cell 6):
   - Загрузка JSON файлов с предсказаниями
   - Подсчет count и area для каждого типа патологии
   - Создание DataFrame с признаками

2. **Создание относительных признаков** (Cell 8):
   - Нормализация по Crypts (count/Crypts_count, area/Crypts_area)
   - Вычисление mean_relative_area
   - Отбор важных признаков

3. **PCA анализ** (Cell 12):
   - StandardScaler для нормализации
   - PCA для снижения размерности
   - Вычисление loadings для интерпретации
   - Создание шкалы PC1_norm (0-1)

4. **Разметка данных** (Cell 11, 13):
   - Группа `mild`: 5 образцов
   - Группа `norma`: 5 образцов
   - Создание меток для сравнения

**Выполнено:**
1. ✅ Создан модуль `aggregate.py`:
   - `aggregate_predictions_from_dict()` - агрегация из словаря предсказаний
   - `aggregate_predictions_from_json()` - агрегация из JSON файла
   - `load_predictions_batch()` - загрузка всех предсказаний из директории
   - `create_relative_features()` - создание относительных признаков (нормализация по Crypts)
   - `select_feature_columns()` - отбор важных признаков

2. ✅ Создан модуль `pca_scoring.py`:
   - Класс `PCAScorer` для PCA анализа и создания шкалы
   - Метод `fit()` - обучение StandardScaler и PCA
   - Метод `transform()` - преобразование данных и создание PC1_norm
   - Метод `fit_transform()` - обучение и преобразование
   - Метод `get_feature_importance()` - получение loadings для интерпретации
   - Методы `save()` и `load()` - сохранение/загрузка модели

3. ✅ Старые README файлы перемещены в `archive/`

**Файлы:**
- `scale/model_config.py` - ✅ создан (конфигурация моделей)
- `scale/aggregate.py` - ✅ создан (агрегация предсказаний)
- `scale/pca_scoring.py` - ✅ создан (PCA анализ и шкала)

**Ресурсы:**
- `models/` - ✅ все 11 моделей YOLO уже присутствуют
- `wsi/` - директория для WSI изображений (пока 1 файл: `17_hp_S008__20240820_084008.tiff`)

---

### Этап 1: Агрегация предсказаний в таблицу признаков ✅

**Статус:** Завершен. Реализовано в `scale/aggregate.py`.

**Цель:** Преобразовать предсказания патологий в таблицу с количественными признаками (count и area для каждого типа патологии).

**Реализовано:**
- ✅ `aggregate_predictions_from_json()` - агрегация из JSON файла
- ✅ `aggregate_predictions_from_dict()` - агрегация из словаря предсказаний
- ✅ `load_predictions_batch()` - загрузка всех предсказаний из директории
- ✅ `create_relative_features()` - создание относительных признаков (нормализация по Crypts)
- ✅ `select_feature_columns()` - отбор важных признаков

**Выходные данные:**
```csv
image,Mild_count,Mild_area,Crypts_count,Crypts_area,...,Dysplasia_count,Dysplasia_area,...
9_ibd_mod_4mod,21,4718158.0,76,12431430.0,...,20,...
```

**Файлы:**
- `scale/aggregate.py` - ✅ модуль для агрегации

---

### Этап 2: Подготовка данных для кластерного анализа ✅ (частично в ноутбуке)

**Статус:** Частично реализовано в `notebook/analyze.ipynb` (Cell 8, 12), требуется расширение.

**Цель:** Подготовить данные для машинного обучения (нормализация, вычисление плотностей, удаление избыточных признаков).

**Что уже сделано:**
- ✅ Создание относительных признаков (нормализация по Crypts)
- ✅ StandardScaler для нормализации (используется в PCA)
- ✅ Обработка NaN (fillna(0))

**Задачи для модуля `preprocessing.py`:**
1. Расширить функционал нормализации:
   - Сохранить метод нормализации по Crypts (как в ноутбуке)
   - Добавить альтернативные методы:
     - Нормализация по total_tissue_area (если доступна)
     - Нормализация по white_space_area
     - MinMaxScaler, RobustScaler

2. Удаление избыточных признаков:
   - Анализ корреляций
   - Удаление признаков с корреляцией > 0.95

3. Сохранение препроцессоров:
   - Pickle для StandardScaler
   - Сохранение списка используемых признаков

4. Утилиты:
   - Функция для анализа корреляций
   - Функция для визуализации распределений признаков

**Файлы:**
- `scale/preprocessing.py` - модуль для препроцессинга (новый)

---

### Этап 3: Разведочный анализ данных (EDA) ⚠️ (частично в ноутбуке)

**Статус:** Базовая разметка данных есть (mild vs norma), но визуализации отсутствуют.

**Цель:** Понять структуру данных, распределения признаков, выявить паттерны.

**Что уже сделано:**
- ✅ Разметка данных на группы (mild, norma)
- ✅ PCA анализ с выявлением важных признаков через loadings
- ❌ Визуализации отсутствуют

**Задачи для модуля `eda.py`:**
1. Визуализация распределений:
   - Гистограммы и boxplots для каждого признака
   - Сравнение mild vs norma (как в ноутбуке)
   - Violin plots по группам

2. Корреляционный анализ:
   - Матрица корреляций (heatmap)
   - Выявление сильно коррелированных признаков

3. Dimensionality reduction визуализация:
   - UMAP/t-SNE с цветовой маркировкой по группам (mild/norma)
   - PCA scatter plot (PC1 vs PC2) с метками групп

4. Статистический анализ:
   - Описательная статистика по группам
   - Тесты значимости различий (t-test, Mann-Whitney)
   - Выявление выбросов

5. Сохранение графиков в `plots/`

**Файлы:**
- `scale/eda.py` - модуль для EDA (новый)
- `plots/` - папка для сохранения графиков

**Зависимости:**
- umap-learn
- seaborn
- matplotlib
- scipy (для статистических тестов)

---

### Этап 4: Кластеризация

**Цель:** Выявить скрытые паттерны в данных и идентифицировать патологические фенотипы.

**Задачи:**
1. Создать модуль `clustering.py`:
   - Реализация HDBSCAN кластеризации (не требует фиксированного числа кластеров)
   - Альтернативные методы: Agglomerative Clustering, KMeans (для сравнения)
   - Визуализация кластеров в UMAP-пространстве
   - Анализ средних значений признаков в каждом кластере
   - Интерпретация кластеров:
     - Кластер 0: высокие нейтрофилы + крипты → активное воспаление
     - Кластер 1: высокие плазматические клетки + эозинофилы → аллергия/EoE
     - Кластер 2: дисплазия + метаплазия → премалигнантное состояние
     - Нормальные биоптаты → компактный кластер с низкими значениями
   - Сохранение модели кластеризации

2. Валидация кластеров:
   - Сравнение с известными диагнозами (если доступны)
   - Метрики качества кластеризации (silhouette score, etc.)

**Файлы:**
- `scale/clustering.py` - новый модуль для кластеризации
- `models/clusterer.pkl` - сохраненная модель кластеризации

**Зависимости:**
- hdbscan

---

### Этап 5: Построение шкалы 0-1 ✅ (расширено спектральным анализом)

**Статус:** Базовая PCA шкала реализована, добавлен спектральный анализ для универсальности.

**Цель:** Создать интерпретируемую шкалу оценки патологии от 0 (норма) до 1 (максимальное отклонение).

**Что уже сделано:**
- ✅ PCA анализ с созданием PC1_norm (нормализованная PC1 от 0 до 1)
- ✅ Выявление важных признаков через loadings
- ✅ Разметка данных (mild/norma) для валидации
- ✅ Спектральный анализ для выявления стабильных состояний (мод)

**Реализовано:**

1. **Модуль `pca_scoring.py`** (базовый PCA):
   - Класс `PCAScorer`:
     - `fit()` - обучение StandardScaler и PCA
     - `transform()` - преобразование данных и создание PC1_norm
     - `get_feature_importance()` - получение loadings для интерпретации
   - Сохранение модели (scaler + pca)

2. **Модуль `spectral_analysis.py`** ✅ (расширенный спектральный анализ):
   - Класс `SpectralAnalyzer`:
     - `fit_pca()` - обучение PCA для снижения размерности
     - `transform_pca()` - преобразование через PCA
     - `fit_spectrum()` - анализ спектра с использованием процентилей (P1, P99)
     - `_find_modes_kde()` - поиск мод через Kernel Density Estimation
     - `fit_gmm()` - обучение Gaussian Mixture Model для моделирования состояний
     - `transform_to_spectrum()` - преобразование в спектральную шкалу 0-1
     - `get_spectrum_info()` - получение информации о спектре
     - `visualize_spectrum()` - визуализация спектра на шкале
     - `save()` и `load()` - сохранение/загрузка модели
   
   **Особенности:**
   - Использование процентилей (P1, P99) для буферных зон
   - Автоматическое выявление мод (стабильных состояний) через KDE
   - Поддержка GMM для моделирования смеси состояний
   - Визуализация спектра с отображением мод на шкале 0-1

**Вариант A: Anomaly Detection** (для сравнения)
1. Модуль `anomaly_detection.py`:
   - Обучение на нормальных данных (unsupervised):
     - Isolation Forest
     - One-Class SVM
     - Autoencoder (MAE reconstruction loss)
   - Вычисление anomaly score → нормализация в [0, 1]
   - Сравнение с PCA методом

**Вариант B: Supervised Regression** (если есть разметка)
1. Модуль `regression.py`:
   - LightGBM Regressor
   - SHAP для интерпретации
   - Сравнение с PCA методом

**Вариант C: Кластер → Score Mapping** (после кластеризации)
1. Модуль `cluster_scoring.py`:
   - Маппинг кластеров на score (0.0 - 1.0)
   - Интеграция с результатами кластеризации

**Рекомендация:** 
- Начать с переноса PCA метода из ноутбука (уже работает)
- Добавить кластеризацию (Этап 4) → Вариант C
- Затем сравнить с Anomaly Detection (Вариант A)

**Файлы:**
- `scale/pca_scoring.py` - ✅ базовая PCA шкала
- `scale/spectral_analysis.py` - ✅ спектральный анализ с модами (расширенный)
- `scale/anomaly_detection.py` - вариант A (новый, опционально)
- `scale/regression.py` - вариант B (новый, опционально)
- `scale/cluster_scoring.py` - вариант C (новый, опционально)
- `models/spectral_analyzer.pkl` - сохраненная модель спектрального анализа

**Зависимости:**
- lightgbm (для варианта B)
- shap (для интерпретации)

---

### Этап 6: Интерпретация и визуализация ✅ (дашборд создан)

**Статус:** Веб-дашборд создан, интерпретация через SHAP - опционально.

**Цель:** Создать инструменты для интерпретации результатов и визуализации.

**Реализовано:**

1. **Модуль `dashboard.py`** ✅ (веб-интерфейс на Streamlit):
   - Загрузка JSON файлов с предсказаниями через веб-интерфейс
   - Агрегация данных и создание признаков
   - Визуализация распределений признаков
   - Спектральный анализ с визуализацией
   - Отображение важности признаков (PC1 loadings)
   - Корреляционная матрица
   - Сохранение экспериментов в директорию `experiments/`
   - Скачивание результатов в CSV
   - Настройки через боковую панель:
     - Использование относительных признаков
     - Спектральный анализ
     - Настройка процентилей


**Задачи (опционально):**
1. Модуль `interpretation.py`:
   - SHAP значения для важности признаков
   - Визуализация вклада каждого признака в итоговый score
   - Генерация объяснений для каждого предсказания

**Файлы:**
- `scale/dashboard.py` - ✅ веб-дашборд (Streamlit)
- `scale/interpretation.py` - интерпретация через SHAP (опционально)

**Зависимости:**
- streamlit ✅
- matplotlib ✅
- shap (опционально, для расширенной интерпретации)

**Использование:**
```bash
# Установка зависимостей
pip install streamlit matplotlib

# Запуск дашборда
streamlit run scale/dashboard.py
```

---

### Этап 7: Интеграция и тестирование

**Цель:** Интегрировать все модули в единый pipeline.

**Задачи:**
1. Создать главный скрипт `analyze.py`:
   - Pipeline от предсказаний до получения score
   - Обработка одного или множества WSI
   - Сохранение результатов

2. Создать примеры использования:
   - `examples/basic_analysis.py` - базовый пример
   - `examples/batch_processing.py` - обработка множества файлов

3. Тестирование:
   - Unit тесты для ключевых функций
   - Интеграционные тесты для pipeline

**Файлы:**
- `scale/analyze.py` - главный скрипт анализа
- `examples/` - папка с примерами
- `tests/` - папка с тестами

---

## Структура файлов после реализации

```
scale/
├── __init__.py
├── domain.py              ✅ (существует)
├── aimodels.py            ✅ (существует)
├── wsi.py                 ✅ (существует)
├── predict.py             ✅ (существует)
├── visualize.py           ✅ (существует)
├── model_config.py        ✅ (создан)
├── aggregate.py           ✅ (создан)
├── pca_scoring.py         ✅ (создан)
├── spectral_analysis.py   ✅ (создан)
├── dashboard.py           ✅ (создан)
├── preprocessing.py       ❌ (новый)
├── eda.py                 ❌ (новый)
├── clustering.py          ❌ (новый)
├── cluster_scoring.py     ❌ (новый, опционально)
├── anomaly_detection.py   ❌ (новый, опционально)
├── regression.py          ❌ (новый, опционально)
├── interpretation.py      ❌ (новый, опционально)
└── analyze.py             ❌ (новый)

examples/
├── basic_analysis.py      ❌ (новый)
└── batch_processing.py    ❌ (новый)

experiments/                ✅ (создается автоматически)

models/
├── *.pt                   ✅ (существуют)
├── clusterer.pkl          ❌ (новый)
└── score_predictor.pkl    ❌ (новый)

plots/                     ❌ (новая папка)
tests/                     ❌ (новая папка)
```

---

## Зависимости для установки

```bash
pip install pandas numpy scikit-learn umap-learn hdbscan lightgbm shap seaborn matplotlib plotly
```

Опционально:
```bash
pip install streamlit  # для дашборда
```

---

## Порядок реализации (рекомендуемый)

0. **Этап 0** - Перенос кода из ноутбуков ✅ **ЗАВЕРШЕН**
   - ✅ Создан `aggregate.py` - агрегация предсказаний и создание относительных признаков
   - ✅ Создан `pca_scoring.py` - PCA анализ и создание шкалы PC1_norm
   - ✅ Старые README перемещены в архив

1. **Этап 1** - Агрегация предсказаний ✅ **ЗАВЕРШЕН**
   - ✅ Реализовано в `aggregate.py`

2. **Этап 2** - Подготовка данных ✅ (частично в ноутбуке)
   - Перенос создания относительных признаков (Cell 8)
   - Расширение функционала нормализации

3. **Этап 3** - EDA ⚠️ (базовая разметка есть, визуализации нет)
   - Создание визуализаций для сравнения mild vs norma
   - Корреляционный анализ
   - UMAP/t-SNE визуализация

4. **Этап 4** - Кластеризация ❌ (новое)
   - HDBSCAN кластеризация
   - Интерпретация кластеров

5. **Этап 5** - Построение шкалы ✅ **РАСШИРЕН**
   - ✅ Базовый PCA метод в `pca_scoring.py`
   - ✅ Спектральный анализ в `spectral_analysis.py` (моды, процентили, визуализация)
   - Опционально: альтернативные методы (anomaly detection, cluster scoring)

6. **Этап 6** - Интерпретация и визуализация ✅ **ДАШБОРД СОЗДАН**
   - ✅ Веб-интерфейс на Streamlit для загрузки данных и визуализации
   - ✅ Сохранение экспериментов
   - Опционально: SHAP значения для расширенной интерпретации

7. **Этап 7** - Интеграция ❌ (новое)
   - Объединение всех модулей в pipeline
   - Создание главного скрипта `analyze.py`

---

## Важные замечания

1. **Нет площади ткани на слайде?** → Использовать нормализацию по максимальному признаку или `area / white_space_area` как proxy.

2. **Мало данных «нормы»?** → Использовать semi-supervised подход: норма = кластер с минимальной активностью всех маркеров.

3. **Нет ground truth диагнозов?** → Начать с unsupervised кластеризации → показать патоморфологу репрезентативные слайды из каждого кластера → получить разметку.

4. **Калибровка модели:** После вердикта патоморфолога можно обновлять модель (online learning).

---

## Следующие шаги

1. Подготовить CSV таблицу с данными (count/area для каждого признака) - использовать `aggregate.py`
2. Провести разведочный анализ данных - использовать `eda.py`
3. Реализовать кластеризацию (HDBSCAN) - использовать `clustering.py`
4. Построить шкалу оценки патологии - использовать `cluster_scoring.py`
5. Создать интерпретируемый дашборд - использовать `interpretation.py` и `dashboard.py`


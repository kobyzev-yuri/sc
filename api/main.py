"""
FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç–æ–ª–æ–≥–∏–π Whole Slide Images.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API —Å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π Streamlit dashboard:
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ê–≥—Ä–µ–≥–∞—Ü–∏—è predictions –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- PCA –∞–Ω–∞–ª–∏–∑ –∏ scoring
- –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- Feature selection –∏ evaluation
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import pandas as pd
import numpy as np
from pathlib import Path
import json
import logging
from datetime import datetime
import io

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ scale –º–æ–¥—É–ª–µ–π
from scale import aggregate, pca_scoring, spectral_analysis, domain
from scale.dashboard_common import load_predictions_from_gdrive
from scale.gcs_integration import load_json_from_gcs_bucket
from scale import dashboard_experiment_selector
from model_development.feature_selection_automated import evaluate_feature_set, identify_sample_type

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
# –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è google –±–∏–±–ª–∏–æ—Ç–µ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ WARNING, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
logging.getLogger('google').setLevel(logging.WARNING)
logging.getLogger('googleapiclient').setLevel(logging.WARNING)
logging.getLogger('google.auth').setLevel(logging.WARNING)

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Pathology Analysis API",
    description="API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç–æ–ª–æ–≥–∏–π Whole Slide Images",
    version="1.0.0"
)

# CORS middleware –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤/–æ—Ç–≤–µ—Ç–æ–≤
class LoadDataRequest(BaseModel):
    source: str = Field(..., description="–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: 'directory', 'gdrive', 'gcs', 'experiment'")
    path: Optional[str] = Field(None, description="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–ª–∏ URL/bucket")
    bucket_name: Optional[str] = Field(None, description="–ò–º—è GCS bucket (–¥–ª—è source='gcs')")
    prefix: Optional[str] = Field("", description="Prefix –¥–ª—è GCS (–¥–ª—è source='gcs')")
    experiment_name: Optional[str] = Field(None, description="–ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–¥–ª—è source='experiment')")
    experiments_dir: Optional[str] = Field("experiments", description="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏")


class AggregateRequest(BaseModel):
    cache_key: str = Field(..., description="Cache key –∏–∑ /api/v1/load-data")


class FeatureEvaluationRequest(BaseModel):
    df_features_cache_key: str = Field(..., description="Cache key –¥–ª—è df_features –∏–∑ /api/v1/aggregate")
    feature_columns: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    mod_samples: Optional[List[str]] = Field(None, description="–°–ø–∏—Å–æ–∫ mod –æ–±—Ä–∞–∑—Ü–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)")
    normal_samples: Optional[List[str]] = Field(None, description="–°–ø–∏—Å–æ–∫ normal –æ–±—Ä–∞–∑—Ü–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)")


class PCAScoreRequest(BaseModel):
    df_features_cache_key: str = Field(..., description="Cache key –¥–ª—è df_features –∏–∑ /api/v1/aggregate")
    feature_columns: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è PCA")
    use_relative_features: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")


class SpectralAnalysisRequest(BaseModel):
    df_features_cache_key: str = Field(..., description="Cache key –¥–ª—è df_features –∏–∑ /api/v1/aggregate")
    feature_columns: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    use_relative_features: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    percentile_low: float = Field(1.0, description="–ù–∏–∂–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å")
    percentile_high: float = Field(99.0, description="–í–µ—Ä—Ö–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å")
    use_gmm_classification: bool = Field(False, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GMM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞")


class LoadExperimentRequest(BaseModel):
    experiment_name: str = Field(..., description="–ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–ª–∏ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    experiments_dir: Optional[str] = Field("experiments", description="–ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏")


class CreateSpectrumRequest(BaseModel):
    df_features_cache_key: str = Field(..., description="Cache key –¥–ª—è df_features –∏–∑ /api/v1/aggregate")
    feature_columns: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è PCA")
    percentile_low: float = Field(1.0, description="–ù–∏–∂–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å")
    percentile_high: float = Field(99.0, description="–í–µ—Ä—Ö–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å")
    use_relative_features: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    use_gmm_classification: bool = Field(False, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GMM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞")


class SaveExperimentRequest(BaseModel):
    experiment_name: str = Field(..., description="–ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    df_features_cache_key: str = Field(..., description="Cache key –¥–ª—è df_features –∏–∑ /api/v1/aggregate")
    feature_columns: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    metrics: Optional[Dict[str, float]] = Field(None, description="–ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (score, separation, etc.)")
    use_relative_features: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    method: str = Field("api_manual", description="–ú–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    experiments_dir: str = Field("experiments", description="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")


# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis –∏–ª–∏ –ë–î)
_data_cache: Dict[str, Any] = {}


# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
try:
    from pathlib import Path
    static_dir = Path(__file__).parent / "static"
    if static_dir.exists():
        app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
except Exception as e:
    logger.warning(f"Could not mount static files: {e}")


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± API –∏–ª–∏ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º."""
    try:
        from pathlib import Path
        static_file = Path(__file__).parent / "static" / "index.html"
        if static_file.exists():
            return FileResponse(str(static_file))
    except Exception:
        pass
    
    return {
        "name": "Pathology Analysis API",
        "version": "1.0.0",
        "endpoints": {
            "load_data": "/api/v1/load-data",
            "aggregate": "/api/v1/aggregate",
            "evaluate_features": "/api/v1/evaluate-features",
            "pca_score": "/api/v1/pca-score",
            "spectral_analysis": "/api/v1/spectral-analysis",
            "create_spectrum": "/api/v1/create-spectrum",
            "list_experiments": "/api/v1/experiments",
            "load_experiment": "/api/v1/load-experiment",
            "save_experiment": "/api/v1/save-experiment",
            "load_progress": "/api/v1/load-progress",
            "download_csv": "/api/v1/download-csv",
            "health": "/api/v1/health",
        },
        "web_interface": "/static/index.html"
    }


@app.post("/api/v1/load-data")
async def load_data(request: LoadDataRequest):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    
    Sources:
    - 'directory': –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    - 'gdrive': –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Google Drive
    - 'gcs': –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Google Cloud Storage
    """
    try:
        predictions = {}
        
        if request.source == "directory":
            if not request.path:
                raise HTTPException(status_code=400, detail="Path required for directory source")
            
            predictions_dir = Path(request.path)
            if not predictions_dir.exists():
                raise HTTPException(status_code=404, detail=f"Directory not found: {request.path}")
            
            json_files = list(predictions_dir.glob("*.json"))
            if not json_files:
                raise HTTPException(status_code=404, detail=f"No JSON files found in {request.path}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥, —á—Ç–æ –∏ dashboard: domain.predictions_from_json()
            # –û–Ω –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict[str, list[Prediction]] (–æ–±—ä–µ–∫—Ç—ã Prediction)
            predictions_converted = {}
            total_files = len(json_files)
            
            # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
            progress_key = "load_progress_latest"
            progress_data = {"current": 0, "total": total_files, "message": f"–ù–∞–π–¥–µ–Ω–æ {total_files} —Ñ–∞–π–ª–æ–≤", "status": "loading", "progress": 0.0}
            _data_cache[progress_key] = progress_data.copy()  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ä–∞–∑—É
            
            def log_progress(current, total, message=""):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∫—ç—à –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥—Ä—É–≥–æ–π endpoint
                progress_data["current"] = current
                progress_data["total"] = total
                progress_data["message"] = message
                progress_data["progress"] = current / total if total > 0 else 0
                _data_cache[progress_key] = progress_data.copy()
            
            log_progress(0, total_files, f"–ù–∞–π–¥–µ–Ω–æ {total_files} —Ñ–∞–π–ª–æ–≤")
            
            for idx, json_file in enumerate(json_files):
                try:
                    preds = domain.predictions_from_json(str(json_file))
                    image_name = json_file.stem
                    predictions_converted[image_name] = preds
                    log_progress(idx + 1, total_files, f"–ó–∞–≥—Ä—É–∂–µ–Ω {json_file.name} ({idx + 1}/{total_files})")
                except Exception as e:
                    logger.error(f"Error loading {json_file.name}: {e}")
                    log_progress(idx + 1, total_files, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {json_file.name}")
                    continue
            
            # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if progress_key in _data_cache:
                del _data_cache[progress_key]
            
            # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if progress_key in _data_cache:
                del _data_cache[progress_key]
        
        elif request.source == "gdrive":
            if not request.path:
                raise HTTPException(status_code=400, detail="Google Drive URL required")
            
            import time
            start_time = time.time()
            progress_key = "load_progress_latest"
            
            try:
                logger.info(f"[GDRIVE] –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Google Drive. URL: {request.path}")
                
                # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
                progress_data = {"current": 0, "total": 0, "message": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Drive...", "status": "loading", "progress": 0.0}
                _data_cache[progress_key] = progress_data.copy()  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ä–∞–∑—É
                
                def log_progress(message):
                    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    logger.info(f"[GDRIVE] Progress: {message}")
                    # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π —Ç–∏–ø–∞ "[1/36]" –∏–ª–∏ "üì• –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É 36 JSON —Ñ–∞–π–ª–æ–≤..."
                    import re
                    progress_match = re.search(r'\[(\d+)/(\d+)\]', message)
                    if progress_match:
                        current = int(progress_match.group(1))
                        total = int(progress_match.group(2))
                        progress_data["current"] = current
                        progress_data["total"] = total
                        progress_data["message"] = message
                        progress_data["progress"] = current / total if total > 0 else 0
                        _data_cache[progress_key] = progress_data.copy()
                    elif "–ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É" in message or "–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏" in message:
                        total_match = re.search(r'(\d+)\s+JSON —Ñ–∞–π–ª–æ–≤', message)
                        if total_match:
                            total = int(total_match.group(1))
                            progress_data["total"] = total
                            progress_data["message"] = message
                            progress_data["progress"] = 0.0
                            _data_cache[progress_key] = progress_data.copy()
                    else:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                        progress_data["message"] = message
                        _data_cache[progress_key] = progress_data.copy()
                
                # –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ folder_id
                logger.info(f"[GDRIVE] –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ folder_id –∏–∑ URL...")
                init_time = time.time()
                try:
                    from scale.dashboard_common import extract_folder_id_from_url
                    folder_id = extract_folder_id_from_url(request.path)
                    elapsed = time.time() - init_time
                    logger.info(f"[GDRIVE] –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. Folder ID: {folder_id}")
                    if elapsed > 5.0:
                        logger.warning(f"[GDRIVE] –≠—Ç–∞–ø 1 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GDRIVE] –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    raise
                
                if not folder_id:
                    error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –ø–∞–ø–∫–∏ –∏–∑ URL"
                    logger.error(f"[GDRIVE] {error_msg}")
                    raise HTTPException(status_code=400, detail=error_msg)
                
                # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ credentials
                logger.info(f"[GDRIVE] –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ credentials...")
                logger.info(f"[GDRIVE] –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
                import os
                has_env_creds = bool(os.getenv('GOOGLE_DRIVE_CREDENTIALS_JSON_B64') or os.getenv('GOOGLE_DRIVE_CREDENTIALS_JSON'))
                logger.info(f"[GDRIVE] Env credentials –Ω–∞–π–¥–µ–Ω—ã: {has_env_creds}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ token —Ñ–∞–π–ª–∞
                token_paths = ['.gdrive_token.json', Path.home() / '.gdrive_token.json']
                token_exists = any(Path(p).exists() for p in token_paths)
                logger.info(f"[GDRIVE] Token —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {token_exists}")
                
                init_time = time.time()
                try:
                    from scale.dashboard_common import get_credentials
                    logger.info(f"[GDRIVE] –í—ã–∑–æ–≤ get_credentials...")
                    credentials = get_credentials(credentials_path=None)
                    elapsed = time.time() - init_time
                    logger.info(f"[GDRIVE] –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. Credentials –ø–æ–ª—É—á–µ–Ω—ã: {credentials is not None}")
                    if elapsed > 10.0:
                        logger.warning(f"[GDRIVE] ‚ö†Ô∏è –≠—Ç–∞–ø 2 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≤–∏—Å–∞–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ credentials")
                    if not credentials:
                        logger.error(f"[GDRIVE] ‚ùå Credentials –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã –ø–æ—Å–ª–µ {elapsed:.2f}—Å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ token —Ñ–∞–π–ª–∞ –∏–ª–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é")
                        logger.error(f"[GDRIVE] Token —Ñ–∞–π–ª—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã: {token_paths}")
                        logger.error(f"[GDRIVE] Env credentials: {has_env_creds}")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GDRIVE] ‚ùå –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    logger.error(f"[GDRIVE] Traceback:", exc_info=True)
                    raise
                
                if not credentials:
                    error_msg = "‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Google Drive"
                    logger.error(f"[GDRIVE] {error_msg}")
                    raise HTTPException(status_code=401, detail=error_msg)
                
                # –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"[GDRIVE] –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–ø–∫–∏...")
                init_time = time.time()
                try:
                    from scale.dashboard_common import load_predictions_from_gdrive
                    predictions_raw = load_predictions_from_gdrive(request.path, log_callback=log_progress)
                    elapsed = time.time() - init_time
                    logger.info(f"[GDRIVE] –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(predictions_raw)}")
                    if elapsed > 60.0:
                        logger.warning(f"[GDRIVE] –≠—Ç–∞–ø 3 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GDRIVE] –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    raise
                
                # –≠—Ç–∞–ø 4: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"[GDRIVE] –≠—Ç–∞–ø 4: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ Prediction –æ–±—ä–µ–∫—Ç—ã...")
                init_time = time.time()
                predictions_converted = {}
                for name, data in predictions_raw.items():
                    predictions_converted[name] = domain.predictions_from_dict(data)
                logger.info(f"[GDRIVE] –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - init_time:.2f}—Å. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(predictions_converted)}")
                
                total_time = time.time() - start_time
                logger.info(f"[GDRIVE] –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f}—Å. –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(predictions_converted)}")
                
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if progress_key in _data_cache:
                    del _data_cache[progress_key]
            except HTTPException:
                raise
            except Exception as e:
                total_time = time.time() - start_time
                logger.error(f"[GDRIVE] –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {total_time:.2f}—Å: {str(e)}", exc_info=True)
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                if progress_key in _data_cache:
                    del _data_cache[progress_key]
                raise HTTPException(status_code=500, detail=f"Error loading from Google Drive: {str(e)}")
        
        elif request.source == "gcs":
            if not request.bucket_name:
                raise HTTPException(status_code=400, detail="Bucket name required for GCS source")
            
            import time
            start_time = time.time()
            progress_key = "load_progress_latest"
            
            try:
                logger.info(f"[GCS] –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ GCS. Bucket: {request.bucket_name}, Prefix: {request.prefix or ''}")
                
                # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
                progress_data = {"current": 0, "total": 0, "message": f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GCS bucket: {request.bucket_name}...", "status": "loading", "progress": 0.0}
                _data_cache[progress_key] = progress_data.copy()  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ä–∞–∑—É
                
                def log_progress(message):
                    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    logger.info(f"[GCS] Progress: {message}")
                    # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π —Ç–∏–ø–∞ "[1/36]" –∏–ª–∏ "üì• –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É 36 JSON —Ñ–∞–π–ª–æ–≤..."
                    import re
                    progress_match = re.search(r'\[(\d+)/(\d+)\]', message)
                    if progress_match:
                        current = int(progress_match.group(1))
                        total = int(progress_match.group(2))
                        progress_data["current"] = current
                        progress_data["total"] = total
                        progress_data["message"] = message
                        progress_data["progress"] = current / total if total > 0 else 0
                        _data_cache[progress_key] = progress_data.copy()
                    elif "–ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É" in message or "–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏" in message or "–ù–∞–π–¥–µ–Ω–æ" in message:
                        total_match = re.search(r'(\d+)\s+JSON —Ñ–∞–π–ª–æ–≤|(\d+)\s+—Ñ–∞–π–ª–æ–≤', message)
                        if total_match:
                            total = int(total_match.group(1) or total_match.group(2))
                            progress_data["total"] = total
                            progress_data["message"] = message
                            progress_data["progress"] = 0.0
                            _data_cache[progress_key] = progress_data.copy()
                    else:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                        progress_data["message"] = message
                        _data_cache[progress_key] = progress_data.copy()
                
                # –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ GCS –∫–ª–∏–µ–Ω—Ç–∞
                logger.info(f"[GCS] –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ GCS –∫–ª–∏–µ–Ω—Ç–∞...")
                init_time = time.time()
                try:
                    from scale.gcs_integration import _get_gcs_client
                    gcs_client = _get_gcs_client(log_callback=lambda m: logger.info(f"[GCS] Client init: {m}"))
                    elapsed = time.time() - init_time
                    logger.info(f"[GCS] –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. Client —Å–æ–∑–¥–∞–Ω: {gcs_client is not None}")
                    if elapsed > 10.0:
                        logger.warning(f"[GCS] –≠—Ç–∞–ø 1 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≤–∏—Å–∞–Ω–∏–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞")
                    if not gcs_client:
                        logger.error(f"[GCS] GCS –∫–ª–∏–µ–Ω—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω –ø–æ—Å–ª–µ {elapsed:.2f}—Å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ credentials")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GCS] –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    raise
                
                if not gcs_client:
                    error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å GCS –∫–ª–∏–µ–Ω—Ç"
                    logger.error(f"[GCS] {error_msg}")
                    raise HTTPException(status_code=500, detail=error_msg)
                
                # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
                logger.info(f"[GCS] –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ bucket...")
                init_time = time.time()
                try:
                    from scale.gcs_integration import list_files_from_gcs_bucket
                    files = list_files_from_gcs_bucket(
                        request.bucket_name,
                        prefix=request.prefix or "",
                        file_type='json',
                        log_callback=lambda m: logger.info(f"[GCS] List files: {m}")
                    )
                    elapsed = time.time() - init_time
                    logger.info(f"[GCS] –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files) if files else 0}")
                    if elapsed > 30.0:
                        logger.warning(f"[GCS] –≠—Ç–∞–ø 2 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≤–∏—Å–∞–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GCS] –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    raise
                
                if not files:
                    error_msg = "‚ö†Ô∏è JSON —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º bucket/prefix"
                    logger.warning(f"[GCS] {error_msg}")
                    raise HTTPException(status_code=404, detail=error_msg)
                
                # –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"[GCS] –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ bucket...")
                init_time = time.time()
                try:
                    from scale.gcs_integration import load_json_from_gcs_bucket
                    predictions_raw = load_json_from_gcs_bucket(
                        request.bucket_name,
                        prefix=request.prefix or "",
                        log_callback=log_progress
                    )
                    elapsed = time.time() - init_time
                    logger.info(f"[GCS] –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}—Å. –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(predictions_raw)}")
                    if elapsed > 60.0:
                        logger.warning(f"[GCS] –≠—Ç–∞–ø 3 –∑–∞–Ω—è–ª {elapsed:.2f}—Å - –≤–æ–∑–º–æ–∂–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
                except Exception as e:
                    elapsed = time.time() - init_time
                    logger.error(f"[GCS] –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}", exc_info=True)
                    raise
                
                # –≠—Ç–∞–ø 4: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"[GCS] –≠—Ç–∞–ø 4: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ Prediction –æ–±—ä–µ–∫—Ç—ã...")
                init_time = time.time()
                predictions_converted = {}
                for name, data in predictions_raw.items():
                    predictions_converted[name] = domain.predictions_from_dict(data)
                logger.info(f"[GCS] –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - init_time:.2f}—Å. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(predictions_converted)}")
                
                total_time = time.time() - start_time
                logger.info(f"[GCS] –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f}—Å. –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(predictions_converted)}")
                
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if progress_key in _data_cache:
                    del _data_cache[progress_key]
            except HTTPException:
                raise
            except Exception as e:
                total_time = time.time() - start_time
                logger.error(f"[GCS] –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {total_time:.2f}—Å: {str(e)}", exc_info=True)
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                if progress_key in _data_cache:
                    del _data_cache[progress_key]
                raise HTTPException(status_code=500, detail=f"Error loading from GCS: {str(e)}")
        
        elif request.source == "experiment":
            if not request.experiment_name:
                raise HTTPException(status_code=400, detail="Experiment name required for experiment source")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–∫–∞–∫ –≤ dashboard)
                experiments_dir = Path(request.experiments_dir)
                experiment_dir = experiments_dir / request.experiment_name
                
                if not experiment_dir.exists():
                    raise HTTPException(status_code=404, detail=f"Experiment not found: {request.experiment_name}")
                
                # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ (–∫–∞–∫ –≤ dashboard)
                aggregated_files = sorted(experiment_dir.glob("aggregated_data_*.csv"))
                relative_files = sorted(experiment_dir.glob("relative_features_*.csv"))
                all_features_files = sorted(experiment_dir.glob("all_features_*.csv"))
                
                if not (aggregated_files or relative_files or all_features_files):
                    raise HTTPException(status_code=404, detail=f"No data files found in experiment: {request.experiment_name}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º all_features –∏–ª–∏ relative_features (–∫–∞–∫ –≤ dashboard)
                if all_features_files:
                    df_from_experiment = pd.read_csv(all_features_files[-1])
                elif relative_files:
                    df_from_experiment = pd.read_csv(relative_files[-1])
                elif aggregated_files:
                    df_from_experiment = pd.read_csv(aggregated_files[-1])
                else:
                    raise HTTPException(status_code=404, detail="No data files found")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                # –ù–ï –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ predictions, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —É–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                experiment_cache_key = f"experiment_{request.experiment_name}_df_features"
                experiment_df_cache_key = f"experiment_{request.experiment_name}_df"
                
                _data_cache[experiment_cache_key] = df_from_experiment.to_dict(orient="records")
                
                if aggregated_files:
                    df_aggregated = pd.read_csv(aggregated_files[-1])
                    _data_cache[experiment_df_cache_key] = df_aggregated.to_dict(orient="records")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (best_features_*.json)
                experiment_config = None
                best_features_files = sorted(experiment_dir.glob("best_features_*.json"))
                if best_features_files:
                    try:
                        with open(best_features_files[-1], 'r', encoding='utf-8') as f:
                            config = json.load(f)
                        experiment_config = {
                            'selected_features': config.get('selected_features', []),
                            'method': config.get('method', 'unknown'),
                            'metrics': config.get('metrics', {}),
                            'timestamp': config.get('timestamp', ''),
                        }
                    except Exception as e:
                        logger.warning(f"Could not load experiment config: {e}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º PCA –¥–∞–Ω–Ω—ã–µ (results.csv), –µ—Å–ª–∏ –µ—Å—Ç—å
                pca_data = None
                pca_cache_key = None
                results_files = sorted(experiment_dir.glob("results.csv"))
                if results_files:
                    try:
                        df_results = pd.read_csv(results_files[-1])
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ PCA –¥–∞–Ω–Ω—ã–µ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ PC1, PC1_norm –∏–ª–∏ image)
                        if 'PC1' in df_results.columns or 'image' in df_results.columns:
                            pca_data = df_results.to_dict(orient="records")
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º PCA –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
                            pca_cache_key = f"{experiment_cache_key}_pca"
                            _data_cache[pca_cache_key] = pca_data
                            logger.info(f"[EXPERIMENT] –ó–∞–≥—Ä—É–∂–µ–Ω—ã PCA –¥–∞–Ω–Ω—ã–µ: {len(pca_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {results_files[-1]}")
                        else:
                            logger.warning(f"[EXPERIMENT] results.csv –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç PCA –¥–∞–Ω–Ω—ã—Ö (–∫–æ–ª–æ–Ω–∫–∏: {list(df_results.columns)})")
                    except Exception as e:
                        logger.warning(f"Could not load PCA data: {e}", exc_info=True)
                else:
                    logger.info(f"[EXPERIMENT] results.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {request.experiment_name}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ analyzer (spectral_analyzer.pkl)
                has_analyzer = (experiment_dir / "spectral_analyzer.pkl").exists()
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                response_data = {
                    "status": "success",
                    "source": request.source,
                    "experiment_name": request.experiment_name,
                    "files_count": len(df_from_experiment),
                    "cache_key": experiment_cache_key,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    "df_features_cache_key": experiment_cache_key,  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª—é—á –¥–ª—è features
                    "df_cache_key": experiment_df_cache_key if aggregated_files else None,
                    "has_aggregated": len(aggregated_files) > 0,
                    "has_features": len(relative_files) > 0 or len(all_features_files) > 0,
                    "sample_names": df_from_experiment['image'].tolist()[:10] if 'image' in df_from_experiment.columns else []
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                if experiment_config:
                    response_data['experiment_config'] = experiment_config
                    response_data['selected_features'] = experiment_config.get('selected_features', [])
                    response_data['metrics'] = experiment_config.get('metrics', {})
                    response_data['method'] = experiment_config.get('method', 'unknown')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ PCA –¥–∞–Ω–Ω—ã—Ö
                if pca_data:
                    response_data['has_pca'] = True
                    response_data['pca_cache_key'] = f"{experiment_cache_key}_pca"
                    response_data['pca_samples_count'] = len(pca_data)
                else:
                    response_data['has_pca'] = False
                
                response_data['has_analyzer'] = has_analyzer
                
                return response_data
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Error loading from experiment: {str(e)}")
        
        else:
            raise HTTPException(status_code=400, detail=f"Unknown source: {request.source}")
        
        # –î–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç experiment, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º predictions
        if request.source != "experiment":
            if not predictions_converted:
                raise HTTPException(status_code=404, detail="No predictions loaded")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            cache_key = f"predictions_{request.source}_{request.path or request.bucket_name}"
            _data_cache[cache_key] = predictions_converted
            
            return {
                "status": "success",
                "source": request.source,
                "files_count": len(predictions_converted),
                "cache_key": cache_key,
                "sample_names": list(predictions_converted.keys())[:10]  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in load_data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/aggregate")
async def aggregate_predictions(request: AggregateRequest):
    """
    –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ predictions –∏ —Å–æ–∑–¥–∞–µ—Ç DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.
    
    –¢—Ä–µ–±—É–µ—Ç cache_key –∏–∑ /api/v1/load-data.
    –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    cache_key = request.cache_key
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (—É–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        if cache_key.startswith("experiment_") and cache_key.endswith("_df_features"):
            if cache_key not in _data_cache:
                raise HTTPException(status_code=404, detail=f"Experiment data not found: {cache_key}")
            
            # –î–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —É–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
            df_features_data = _data_cache[cache_key]
            df_features = pd.DataFrame(df_features_data)
            
            # –ü–æ–ª—É—á–∞–µ–º aggregated data, –µ—Å–ª–∏ –µ—Å—Ç—å
            df_cache_key = cache_key.replace("_df_features", "_df")
            df = None
            if df_cache_key in _data_cache:
                df_data = _data_cache[df_cache_key]
                df = pd.DataFrame(df_data)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç aggregated, —Å–æ–∑–¥–∞–µ–º –∏–∑ features (–æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–µ —Ç–æ—á–Ω–∞—è, –Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                df = df_features.copy()
            
            # –°–æ–∑–¥–∞–µ–º all_features
            df_all_features = aggregate.select_all_feature_columns(df_features)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏
            df_all_features_cache_key = f"{cache_key.replace('_df_features', '')}_df_all"
            _data_cache[df_all_features_cache_key] = df_all_features.to_dict(orient="records")
            
            return {
                "status": "success",
                "aggregated_rows": len(df_features),
                "features_count": len(df_features.columns) - 1,  # -1 –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ 'image'
                "df_cache_key": df_cache_key if df_cache_key in _data_cache else None,
                "df_features_cache_key": cache_key,
                "df_all_features_cache_key": df_all_features_cache_key,
                "feature_columns": [col for col in df_features.columns if col != 'image'],
                "from_experiment": True
            }
        
        # –û–±—ã—á–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –∏–∑ predictions
        if cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Data not found. Load data first using cache_key: {cache_key}")
        
        predictions = _data_cache[cache_key]
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        rows = []
        for image_name, preds in predictions.items():
            pred_stats = aggregate.aggregate_predictions_from_dict(preds, image_name)
            rows.append(pred_stats)
        
        df = pd.DataFrame(rows)
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_features = aggregate.create_relative_features(df)
        
        # –°–æ–∑–¥–∞–µ–º all_features (–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
        df_all_features = aggregate.select_all_feature_columns(df_features)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        df_cache_key = f"{cache_key}_df"
        df_features_cache_key = f"{cache_key}_df_features"
        df_all_features_cache_key = f"{cache_key}_df_all"
        _data_cache[df_cache_key] = df.to_dict(orient="records")
        _data_cache[df_features_cache_key] = df_features.to_dict(orient="records")
        _data_cache[df_all_features_cache_key] = df_all_features.to_dict(orient="records")
        
        return {
            "status": "success",
            "aggregated_rows": len(df),
            "features_count": len(df_features.columns) - 1,  # -1 –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ 'image'
            "df_cache_key": df_cache_key,
            "df_features_cache_key": df_features_cache_key,
            "df_all_features_cache_key": df_all_features_cache_key,
            "feature_columns": [col for col in df_features.columns if col != 'image']
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in aggregate_predictions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/evaluate-features")
async def evaluate_features(request: FeatureEvaluationRequest):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è mod –∏ normal –æ–±—Ä–∞–∑—Ü–æ–≤.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏: score, separation, mean_pc1_norm_mod, explained_variance
    """
    try:
        df_features_cache_key = request.df_features_cache_key
        if df_features_cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Features not found. Run aggregate first.")
        
        df_features_data = _data_cache[df_features_cache_key]
        df_features = pd.DataFrame(df_features_data)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º mod –∏ normal –æ–±—Ä–∞–∑—Ü—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if not request.mod_samples or not request.normal_samples:
            mod_samples = []
            normal_samples = []
            
            if "image" in df_features.columns:
                for img_name in df_features["image"].unique():
                    sample_type = identify_sample_type(str(img_name))
                    if sample_type == 'mod':
                        mod_samples.append(img_name)
                    elif sample_type == 'normal':
                        normal_samples.append(img_name)
        else:
            mod_samples = request.mod_samples
            normal_samples = request.normal_samples
        
        if not mod_samples or not normal_samples:
            raise HTTPException(
                status_code=400,
                detail="No mod or normal samples found. Specify mod_samples and normal_samples explicitly."
            )
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        metrics = evaluate_feature_set(
            df_features,
            request.feature_columns,
            mod_samples,
            normal_samples
        )
        
        return {
            "status": "success",
            "metrics": metrics,
            "mod_samples_count": len(mod_samples),
            "normal_samples_count": len(normal_samples),
            "features_count": len(request.feature_columns)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in evaluate_features: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/pca-score")
async def pca_score(request: PCAScoreRequest):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç PCA score –¥–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç PC1 –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞.
    """
    try:
        df_features_cache_key = request.df_features_cache_key
        if df_features_cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Features not found. Run aggregate first.")
        
        df_features_data = _data_cache[df_features_cache_key]
        df_features = pd.DataFrame(df_features_data)
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if request.use_relative_features:
            df_for_pca = df_features
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å df, –∞ –Ω–µ df_features)
            df_cache_key = df_features_cache_key.replace("_df_features", "_df")
            if df_cache_key not in _data_cache:
                raise HTTPException(status_code=404, detail="Absolute features not found. Use use_relative_features=true")
            df_data = _data_cache[df_cache_key]
            df_for_pca = pd.DataFrame(df_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        missing_features = [f for f in request.feature_columns if f not in df_for_pca.columns]
        if missing_features:
            raise HTTPException(
                status_code=400,
                detail=f"Features not found: {missing_features}"
            )
        
        # –í—ã—á–∏—Å–ª—è–µ–º PCA score –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ –∂–µ –º–µ—Ç–æ–¥—ã, —á—Ç–æ –∏ dashboard
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fit_transform –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        pca_scorer = pca_scoring.PCAScorer()
        df_result = pca_scorer.fit_transform(
            df_for_pca,
            feature_columns=request.feature_columns
        )
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ 'image' (–æ–Ω–∞ –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
        if 'image' in df_result.columns:
            results = df_result[['image', 'PC1', 'PC1_norm']].to_dict(orient="records")
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'image', –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å
            df_result['image'] = df_result.index.astype(str)
            results = df_result[['image', 'PC1', 'PC1_norm']].to_dict(orient="records")
        
        return {
            "status": "success",
            "results": results,
            "samples_count": len(results)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in pca_score: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/create-spectrum")
async def create_spectrum(request: CreateSpectrumRequest):
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ PCA –¥–∞–Ω–Ω—ã—Ö (–∞–ª–∏–∞—Å –¥–ª—è spectral-analysis).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º.
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º CreateSpectrumRequest –≤ SpectralAnalysisRequest
    spectral_request = SpectralAnalysisRequest(
        df_features_cache_key=request.df_features_cache_key,
        feature_columns=request.feature_columns,
        percentile_low=request.percentile_low,
        percentile_high=request.percentile_high,
        use_relative_features=request.use_relative_features,
        use_gmm_classification=request.use_gmm_classification
    )
    # –í—ã–∑—ã–≤–∞–µ–º spectral_analysis_endpoint
    return await spectral_analysis_endpoint(spectral_request)


@app.post("/api/v1/spectral-analysis")
async def spectral_analysis_endpoint(request: SpectralAnalysisRequest):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥—ã, KDE, GMM –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    """
    try:
        df_features_cache_key = request.df_features_cache_key
        if df_features_cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Features not found. Run aggregate first.")
        
        df_features_data = _data_cache[df_features_cache_key]
        df_features = pd.DataFrame(df_features_data)
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if request.use_relative_features:
            df_for_analysis = df_features
        else:
            df_cache_key = df_features_cache_key.replace("_df_features", "_df")
            if df_cache_key not in _data_cache:
                raise HTTPException(status_code=404, detail="Absolute features not found")
            df_data = _data_cache[df_cache_key]
            df_for_analysis = pd.DataFrame(df_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        missing_features = [f for f in request.feature_columns if f not in df_for_analysis.columns]
        if missing_features:
            raise HTTPException(
                status_code=400,
                detail=f"Features not found: {missing_features}"
            )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫–∞–∫ –≤ dashboard)
        analyzer = spectral_analysis.SpectralAnalyzer()
        
        # 1. –û–±—É—á–∞–µ–º PCA
        analyzer.fit_pca(df_for_analysis, request.feature_columns)
        
        # 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ PCA
        df_pca = analyzer.transform_pca(df_for_analysis)
        
        # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–µ–∫—Ç—Ä
        analyzer.fit_spectrum(
            df_pca,
            percentile_low=request.percentile_low,
            percentile_high=request.percentile_high
        )
        
        # 4. –û–±—É—á–∞–µ–º GMM, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if request.use_gmm_classification:
            analyzer.fit_gmm(df_pca)
        
        # 5. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—É—é —à–∫–∞–ª—É (–∫–∞–∫ –≤ dashboard)
        df_spectrum = analyzer.transform_to_spectrum(
            df_pca,
            use_gmm_classification=request.use_gmm_classification
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º analyzer –∏ spectrum –≤ –∫—ç—à –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        spectrum_cache_key = f"{df_features_cache_key}_spectrum"
        analyzer_cache_key = f"{df_features_cache_key}_analyzer"
        _data_cache[spectrum_cache_key] = df_spectrum.to_dict(orient="records")
        _data_cache[analyzer_cache_key] = analyzer  # –°–æ—Ö—Ä–∞–Ω—è–µ–º analyzer –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        modes = analyzer.get_modes()
        kde_data = analyzer.get_kde_data()
        gmm_data = analyzer.get_gmm_data()
        
        return {
            "status": "success",
            "modes": modes,
            "kde": kde_data,
            "gmm": gmm_data,
            "spectrum_data": df_spectrum.to_dict(orient="records"),
            "percentiles": analyzer.pc1_percentiles,
            "samples_count": len(df_pca),
            "spectrum_cache_key": spectrum_cache_key,
            "analyzer_cache_key": analyzer_cache_key
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in spectral_analysis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/experiments")
async def list_experiments():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
    """
    try:
        from scale import dashboard_experiment_selector
        
        experiments = dashboard_experiment_selector.list_available_experiments(
            use_tracker=True,
            top_n=None,
            check_data=True
        )
        
        return {
            "status": "success",
            "experiments": experiments,
            "count": len(experiments)
        }
    
    except Exception as e:
        logger.error(f"Error in list_experiments: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/load-experiment")
async def load_experiment(request: LoadExperimentRequest):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–∞–Ω–∞–ª–æ–≥ load_data —Å source='experiment').
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º.
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ –≤ load_data —Å source='experiment'
        experiments_dir = Path(request.experiments_dir)
        experiment_dir = experiments_dir / request.experiment_name
        
        if not experiment_dir.exists():
            raise HTTPException(status_code=404, detail=f"Experiment not found: {request.experiment_name}")
        
        # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        aggregated_files = sorted(experiment_dir.glob("aggregated_data_*.csv"))
        relative_files = sorted(experiment_dir.glob("relative_features_*.csv"))
        all_features_files = sorted(experiment_dir.glob("all_features_*.csv"))
        
        if not (aggregated_files or relative_files or all_features_files):
            raise HTTPException(status_code=404, detail=f"No data files found in experiment: {request.experiment_name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        if all_features_files:
            df_from_experiment = pd.read_csv(all_features_files[-1])
        elif relative_files:
            df_from_experiment = pd.read_csv(relative_files[-1])
        elif aggregated_files:
            df_from_experiment = pd.read_csv(aggregated_files[-1])
        else:
            raise HTTPException(status_code=404, detail="No data files found")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
        experiment_cache_key = f"experiment_{request.experiment_name}_df_features"
        experiment_df_cache_key = f"experiment_{request.experiment_name}_df"
        
        _data_cache[experiment_cache_key] = df_from_experiment.to_dict(orient="records")
        
        if aggregated_files:
            df_aggregated = pd.read_csv(aggregated_files[-1])
            _data_cache[experiment_df_cache_key] = df_aggregated.to_dict(orient="records")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_config = None
        best_features_files = sorted(experiment_dir.glob("best_features_*.json"))
        if best_features_files:
            try:
                with open(best_features_files[-1], 'r', encoding='utf-8') as f:
                    config = json.load(f)
                experiment_config = {
                    'selected_features': config.get('selected_features', []),
                    'method': config.get('method', 'unknown'),
                    'metrics': config.get('metrics', {}),
                    'timestamp': config.get('timestamp', ''),
                }
            except Exception as e:
                logger.warning(f"Could not load experiment config: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º PCA –¥–∞–Ω–Ω—ã–µ (results.csv), –µ—Å–ª–∏ –µ—Å—Ç—å
        pca_data = None
        pca_cache_key = None
        results_files = sorted(experiment_dir.glob("results.csv"))
        if results_files:
            try:
                df_results = pd.read_csv(results_files[-1])
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ PCA –¥–∞–Ω–Ω—ã–µ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ PC1, PC1_norm –∏–ª–∏ image)
                if 'PC1' in df_results.columns or 'image' in df_results.columns:
                    pca_data = df_results.to_dict(orient="records")
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PCA –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
                    pca_cache_key = f"{experiment_cache_key}_pca"
                    _data_cache[pca_cache_key] = pca_data
                    logger.info(f"[EXPERIMENT] –ó–∞–≥—Ä—É–∂–µ–Ω—ã PCA –¥–∞–Ω–Ω—ã–µ: {len(pca_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {results_files[-1]}")
                else:
                    logger.warning(f"[EXPERIMENT] results.csv –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç PCA –¥–∞–Ω–Ω—ã—Ö (–∫–æ–ª–æ–Ω–∫–∏: {list(df_results.columns)})")
            except Exception as e:
                logger.warning(f"Could not load PCA data: {e}", exc_info=True)
        else:
            logger.info(f"[EXPERIMENT] results.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {request.experiment_name}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –æ–∂–∏–¥–∞–µ–º–æ–º —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
        response_data = {
            "status": "success",
            "experiment_name": request.experiment_name,
            "n_features": len(experiment_config.get('selected_features', [])) if experiment_config else 0,
            "method": experiment_config.get('method', 'unknown') if experiment_config else 'unknown',
            "metrics": experiment_config.get('metrics', {}) if experiment_config else {},
            "features": experiment_config.get('selected_features', []) if experiment_config else [],
            "pca_data": pca_data[:10] if pca_data else [],  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            "has_pca": pca_data is not None and len(pca_data) > 0,
            "pca_cache_key": pca_cache_key,
            "pca_samples_count": len(pca_data) if pca_data else 0,
            "cache_key": experiment_cache_key,
            "df_features_cache_key": experiment_cache_key,
        }
        
        return response_data
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in load_experiment: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error loading experiment: {str(e)}")


@app.get("/api/v1/load-progress")
async def get_load_progress():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
    """
    progress_key = "load_progress_latest"
    if progress_key in _data_cache:
        return {
            "status": "loading",
            **(_data_cache[progress_key])
        }
    return {
        "status": "completed",
        "current": 0,
        "total": 0,
        "progress": 1.0,
        "message": "–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
    }


@app.get("/api/v1/download-csv")
async def download_csv(cache_key: str, filename: Optional[str] = None):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç CSV —Ñ–∞–π–ª –∏–∑ –∫—ç—à–∞.
    
    Args:
        cache_key: Cache key –¥–ª—è –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, df_features_cache_key)
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    try:
        if cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Data not found: {cache_key}")
        
        df_data = _data_cache[cache_key]
        df = pd.DataFrame(df_data)
        
        # –°–æ–∑–¥–∞–µ–º CSV –≤ –ø–∞–º—è—Ç–∏
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        csv_content = csv_buffer.getvalue()
        csv_bytes = csv_content.encode('utf-8')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        if not filename:
            filename = f"{cache_key}.csv"
        if not filename.endswith('.csv'):
            filename += '.csv'
        
        return StreamingResponse(
            io.BytesIO(csv_bytes),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in download_csv: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/save-experiment")
async def save_experiment(request: SaveExperimentRequest):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ dashboard —Å–æ –≤—Å–µ–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏.
    
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç:
    - aggregated_data_{timestamp}.csv
    - relative_features_{timestamp}.csv (–∏–ª–∏ all_features)
    - results.csv (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–ø–µ–∫—Ç—Ä)
    - spectral_analyzer.pkl (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–ø–µ–∫—Ç—Ä)
    - best_features_{timestamp}.json
    - metadata.json
    """
    try:
        from scale.dashboard import create_experiment_dir, save_experiment as dashboard_save_experiment
        from scale import aggregate
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
        if request.df_features_cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail=f"Features not found: {request.df_features_cache_key}")
        
        df_features_data = _data_cache[request.df_features_cache_key]
        df_features = pd.DataFrame(df_features_data)
        
        # –ü–æ–ª—É—á–∞–µ–º aggregated data
        df_cache_key = request.df_features_cache_key.replace("_df_features", "_df")
        if df_cache_key not in _data_cache:
            raise HTTPException(status_code=404, detail="Aggregated data not found. Run aggregate first.")
        
        df_aggregated_data = _data_cache[df_cache_key]
        df_aggregated = pd.DataFrame(df_aggregated_data)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiments_dir = Path(request.experiments_dir)
        exp_dir = create_experiment_dir(experiments_dir)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Ñ–∞–π–ª—ã (–∫–∞–∫ –≤ export_complete_results)
        aggregated_path = exp_dir / f"aggregated_data_{timestamp}.csv"
        df_aggregated.to_csv(aggregated_path, index=False)
        
        features_path = exp_dir / f"relative_features_{timestamp}.csv" if request.use_relative_features else exp_dir / f"all_features_{timestamp}.csv"
        df_features.to_csv(features_path, index=False)
        
        # –ü–æ–ª—É—á–∞–µ–º all_features (–µ—Å–ª–∏ –µ—Å—Ç—å)
        df_all_features = None
        all_features_cache_key = request.df_features_cache_key.replace("_df_features", "_df_all")
        if all_features_cache_key in _data_cache:
            df_all_features_data = _data_cache[all_features_cache_key]
            df_all_features = pd.DataFrame(df_all_features_data)
            all_features_path = exp_dir / f"all_features_{timestamp}.csv"
            df_all_features.to_csv(all_features_path, index=False)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ–∫—Ç—Ä–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        analyzer = None
        df_results = None
        spectrum_cache_key = f"{request.df_features_cache_key}_spectrum"
        analyzer_cache_key = f"{request.df_features_cache_key}_analyzer"
        
        if spectrum_cache_key in _data_cache:
            df_spectrum_data = _data_cache[spectrum_cache_key]
            df_results = pd.DataFrame(df_spectrum_data)
            
            # –ü–æ–ª—É—á–∞–µ–º analyzer –∏–∑ –∫—ç—à–∞
            if analyzer_cache_key in _data_cache:
                analyzer = _data_cache[analyzer_cache_key]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "method": request.method,
            "use_relative_features": request.use_relative_features,
            "n_features": len(request.feature_columns),
            "selected_features": request.feature_columns,
            "timestamp": datetime.now().isoformat(),
            "n_samples": len(df_features)
        }
        
        if request.metrics:
            metadata["metrics"] = request.metrics
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —á–µ—Ä–µ–∑ dashboard —Ñ—É–Ω–∫—Ü–∏—é
        if df_results is not None and analyzer is not None:
            dashboard_save_experiment(
                exp_dir,
                df_results,
                analyzer=analyzer,
                metadata=metadata,
                selected_features=request.feature_columns,
                metrics=request.metrics,
                use_relative_features=request.use_relative_features
            )
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            metadata_path = exp_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º best_features JSON
            if request.feature_columns and request.metrics:
                best_features_path = exp_dir / f"best_features_{timestamp}.json"
                config = {
                    'method': request.method,
                    'selected_features': request.feature_columns,
                    'metrics': {
                        'score': float(request.metrics.get('score', 0)),
                        'separation': float(request.metrics.get('separation', 0)),
                        'mean_pc1_norm_mod': float(request.metrics.get('mean_pc1_norm_mod', 0)),
                        'explained_variance': float(request.metrics.get('explained_variance', 0)),
                    },
                    'timestamp': timestamp,
                    'use_relative_features': request.use_relative_features,
                }
                with open(best_features_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ —Ç—Ä–µ–∫–µ—Ä–µ
        try:
            from model_development.experiment_tracker import ExperimentTracker, register_experiment_from_directory
            tracker = ExperimentTracker(experiments_dir)
            exp_id = register_experiment_from_directory(
                experiment_dir=exp_dir,
                tracker=tracker,
                train_set=metadata.get("train_set", "results/predictions"),
                aggregation_version=metadata.get("aggregation_version", "current"),
            )
        except Exception as e:
            logger.warning(f"Could not register experiment in tracker: {e}")
        
        return {
            "status": "success",
            "experiment_name": request.experiment_name,
            "experiment_dir": str(exp_dir),
            "experiment_path": str(exp_dir),
            "files_saved": {
                "aggregated_data": str(aggregated_path),
                "features": str(features_path),
                "all_features": str(all_features_path) if df_all_features is not None else None,
                "results": str(exp_dir / "results.csv") if df_results is not None else None,
                "spectral_analyzer": str(exp_dir / "spectral_analyzer.pkl") if analyzer is not None else None,
                "best_features": str(exp_dir / f"best_features_{timestamp}.json") if request.feature_columns else None,
                "metadata": str(exp_dir / "metadata.json")
            }
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in save_experiment: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


if __name__ == "__main__":
    import uvicorn
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞–≥—Ä—É–∑–∫–∏
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        timeout_keep_alive=300,  # 5 –º–∏–Ω—É—Ç –¥–ª—è keep-alive
        timeout_graceful_shutdown=300  # 5 –º–∏–Ω—É—Ç –¥–ª—è graceful shutdown
    )

